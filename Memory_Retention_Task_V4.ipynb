{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF5hwlgpdmrX",
        "outputId": "ed8a7c54-e3f5-4aab-f53f-4196c10958ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yguon823eABn"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/gdrive/MyDrive/Memory Retention Research/Datasets/Dataset_10000/Dataset_10000.csv\"\n",
        "train_path = \"/content/gdrive/MyDrive/Memory Retention Research/Datasets/Dataset_5000/train.csv\"\n",
        "test_path = \"/content/gdrive/MyDrive/Memory Retention Research/Datasets/Dataset_5000/test.csv\"\n",
        "val_path = \"/content/gdrive/MyDrive/Memory Retention Research/Datasets/Dataset_5000/validation.csv\"\n",
        "base_path = \"/content/gdrive/MyDrive/Memory Retention Research/Datasets/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ixagE5pccw",
        "outputId": "f32d9be5-fa2e-4cd2-947b-ad8cb5a61d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.36)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.8.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.7.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transliterate in /usr/local/lib/python3.10/dist-packages (1.10.2)\n",
            "Requirement already satisfied: six>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from transliterate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install fairseq\n",
        "!pip install transformers\n",
        "!pip install contractions\n",
        "!pip install sentencepiece\n",
        "!pip install transliterate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8SScCEwq3ov"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp3IwFsFnN17",
        "outputId": "9db9ee55-b9df-409a-cafe-faa77e64bb82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "import keras\n",
        "import nltk\n",
        "import nltk.data\n",
        "import contractions\n",
        "import re\n",
        "import random\n",
        "import copy\n",
        "import string\n",
        "import time, datetime\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import datasets as ds\n",
        "import gensim.downloader as api\n",
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.models import Sequential, Model, clone_model\n",
        "from keras.layers import Input, Concatenate, Dense, Conv1D, GlobalMaxPooling1D, Reshape, Embedding, LSTM, Dropout, Multiply, Dot, Activation\n",
        "from keras.utils import pad_sequences\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam, Adagrad, Adadelta\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import T5Tokenizer, T5Model\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords, words\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCHmTLycKsSE"
      },
      "outputs": [],
      "source": [
        "#for glove\n",
        "glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acRgul3hgWRp"
      },
      "outputs": [],
      "source": [
        "class DatasetCreation():\n",
        "  def __init__(self, glove_model, min_word_size=10000):\n",
        "\n",
        "    self.vocabulary = set()\n",
        "    self.positive_vocabulary = list()\n",
        "    self.stop_words = set(stopwords.words('english'))\n",
        "    self.english_words = set(words.words())\n",
        "    self.min_word_size = min_word_size\n",
        "    self.glove_model = glove_model\n",
        "\n",
        "    with open('bias_lexicon.txt', 'r') as file:\n",
        "      self.bias_words = file.read().splitlines()\n",
        "\n",
        "    self.vad_df = pd.read_csv('NRC-VAD-Lexicon.txt', delimiter='\\t', names=['Words', 'valence', 'arousal', 'dominance'])\n",
        "    self.vad_words = set(self.vad_df['Words'].values)\n",
        "    self.bias_words = set(self.bias_words)\n",
        "\n",
        "    self.vad_bias_words = list(self.vad_words.union(self.bias_words))\n",
        "\n",
        "    self.final_word_mapping = {}\n",
        "\n",
        "    self.vad_bias_dict = {}\n",
        "    for vad_bias in self.vad_bias_words:\n",
        "      self.vad_bias_dict[vad_bias] = []\n",
        "\n",
        "  def load_data(self):\n",
        "    self.dataset = ds.load_dataset('lambada')\n",
        "    self.trainset = self.dataset['train']['text']\n",
        "    self.testset = self.dataset['test']['text']\n",
        "    self.dataset = self.trainset + self.testset\n",
        "    print(\"{} paragraphs.\\n\".format(len(self.dataset)))\n",
        "\n",
        "  def create_paragraphs(self):\n",
        "    times = 2\n",
        "    time = 0\n",
        "    result_cleaned_paragraphs = []\n",
        "    cleaned_paragraphs = self.df['cleaned_paragraph'].values\n",
        "    for paragraph in cleaned_paragraphs:\n",
        "      time = 0\n",
        "      res_paragraph = []\n",
        "      sentences = nltk.sent_tokenize(paragraph)\n",
        "      for sentence in sentences:\n",
        "        tokens = sentence.split()\n",
        "        if(len(res_paragraph) <= self.min_word_size):\n",
        "          res_paragraph.extend(tokens)\n",
        "        else:\n",
        "          result_cleaned_paragraphs.append(' '.join(res_paragraph))\n",
        "          if time < times:\n",
        "            time += 1\n",
        "            res_paragraph = []\n",
        "            res_paragraph.extend(tokens)\n",
        "            continue\n",
        "          else:\n",
        "            break\n",
        "    return result_cleaned_paragraphs\n",
        "\n",
        "\n",
        "  def create_dataframe(self):\n",
        "    self.df = pd.DataFrame(self.dataset, columns=['raw_paragraph'])\n",
        "\n",
        "  def is_meaningful(self, word):\n",
        "    return word in self.english_words and word not in self.stop_words\n",
        "\n",
        "  def clean_paragraph(self, paragraph):\n",
        "    text = re.sub('\\s+', ' ', paragraph.strip())\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "  def create_vocabulary(self, paragraph):\n",
        "    words = nltk.word_tokenize(paragraph)\n",
        "    self.vocabulary.update(words)\n",
        "\n",
        "  def has_negative_word(self, paragraph, positive_word):\n",
        "    if positive_word in self.glove_model.key_to_index:\n",
        "      similar_words = self.glove_model.most_similar(positive_word)\n",
        "      for similar_word, _ in similar_words:\n",
        "        if similar_word not in paragraph:\n",
        "          return True\n",
        "    return False\n",
        "\n",
        "  def complete_vad_bias_dictionary(self, cleaned_paragraphs):\n",
        "    all_len = len(self.vad_bias_dict.keys())\n",
        "    for i, paragraphs in enumerate(cleaned_paragraphs):\n",
        "      tokens = paragraphs.split()\n",
        "      intersect = set(tokens).intersection(self.vad_bias_words)\n",
        "      for vad_bias_word in list(intersect):\n",
        "        self.vad_bias_dict[vad_bias_word].append(i)\n",
        "    empty_keys = [key for key, value in self.vad_bias_dict.items() if len(value) == 0]\n",
        "    for key in empty_keys:\n",
        "      del self.vad_bias_dict[key]\n",
        "    modified_len = len(self.vad_bias_dict.keys())\n",
        "    print(\"{} words are in paragraphs out of {}\".format(modified_len, all_len))\n",
        "\n",
        "  def delete_index_from_all_dictionary(self, index):\n",
        "    delete_these = []\n",
        "    for key, value in self.vad_bias_dict.items():\n",
        "      if index in value:\n",
        "        delete_these.append(key)\n",
        "    for key in delete_these:\n",
        "      self.vad_bias_dict[key].remove(index)\n",
        "\n",
        "    empty_keys = [key for key, value in self.vad_bias_dict.items() if len(value) == 0]\n",
        "    for key in empty_keys:\n",
        "      del self.vad_bias_dict[key]\n",
        "\n",
        "\n",
        "  def assign_vad_bias(self):\n",
        "    sorted_dict = dict(sorted(self.vad_bias_dict.items(), key=lambda item: len(item[1])))\n",
        "    while(len(sorted_dict) != 0):\n",
        "      key = list(sorted_dict.keys())[0]\n",
        "      indexes = sorted_dict[key]\n",
        "      index = random.choice(indexes)\n",
        "      del self.vad_bias_dict[key]\n",
        "      self.final_word_mapping[index] = key\n",
        "      self.delete_index_from_all_dictionary(index)\n",
        "      sorted_dict = dict(sorted(self.vad_bias_dict.items(), key=lambda item: len(item[1])))\n",
        "\n",
        "  def random_positive_word(self, paragraph):\n",
        "    searching = True\n",
        "    dictionary = np.unique(paragraph.split())\n",
        "    while(searching):\n",
        "      positive_word = random.choice(dictionary)\n",
        "      if self.is_meaningful(positive_word) and self.has_negative_word(paragraph, positive_word):\n",
        "        return positive_word\n",
        "\n",
        "\n",
        "  def fill_positive_words(self, paragraphs):\n",
        "    self.positive_words = [0 for _ in range(len(paragraphs))]\n",
        "    for i, paragraph in enumerate(paragraphs):\n",
        "      if i in self.final_word_mapping.keys():\n",
        "        self.positive_words[i] = self.final_word_mapping[i]\n",
        "      else:\n",
        "        self.positive_words[i] = self.random_positive_word(paragraph)\n",
        "\n",
        "  def find_positive_words(self, cleaned_paragraphs):\n",
        "    self.complete_vad_bias_dictionary(cleaned_paragraphs)\n",
        "    self.assign_vad_bias()\n",
        "    self.fill_positive_words(cleaned_paragraphs)\n",
        "\n",
        "  def find_negative_words(self, paragraphs):\n",
        "    self.negative_words = [0 for _ in range(len(self.positive_words))]\n",
        "    for i, paragraph in tqdm(enumerate(paragraphs)):\n",
        "      positive_word = self.positive_words[i]\n",
        "      if positive_word in self.glove_model.key_to_index:\n",
        "        similar_words = self.glove_model.most_similar(positive_word)\n",
        "      else:\n",
        "        continue\n",
        "      for neg_word, _ in similar_words:\n",
        "        if neg_word not in paragraph:\n",
        "          self.negative_words[i] = neg_word\n",
        "          break\n",
        "    for i, negative_word in tqdm(enumerate(self.negative_words)):\n",
        "      if negative_word == 0:\n",
        "        positive_word = self.random_positive_word(paragraph)\n",
        "        similar_words = self.glove_model.most_similar(positive_word)\n",
        "        for neg_word, _ in similar_words:\n",
        "          if neg_word not in paragraph:\n",
        "            self.negative_words[i] = neg_word\n",
        "            break\n",
        "  def drop_duplicates(self):\n",
        "    searching = True\n",
        "    dup = self.df.drop_duplicates(subset=['Target Word'], keep='first')\n",
        "    while (searching):\n",
        "      pos_df = dup[dup['Label']==1]\n",
        "      neg_df = dup[dup['Label']==0]\n",
        "      print(\"we have {} positive unique words\".format(len(pos_df)))\n",
        "      print(\"we have {} negative unique words\".format(len(neg_df)))\n",
        "      if len(pos_df) == len(neg_df):\n",
        "        break\n",
        "\n",
        "      if len(neg_df) < len(pos_df):\n",
        "        pos_df = pos_df[pos_df['Paragraph'].isin(neg_df['Paragraph'])]\n",
        "      else:\n",
        "        neg_df = neg_df[neg_df['Paragraph'].isin(pos_df['Paragraph'])]\n",
        "      dup = pd.concat([pos_df, neg_df])\n",
        "    self.df = dup\n",
        "\n",
        "  def save(self):\n",
        "    base_path = \"/content/gdrive/MyDrive/Memory Retention Research/Datasets/Dataset_{}/Dataset_{}.csv\".format(self.min_word_size, self.min_word_size)\n",
        "    self.df.to_csv(base_path, index=False)\n",
        "\n",
        "  def create(self):\n",
        "    print(\"Creating..\")\n",
        "    self.df['cleaned_paragraph'] = self.df['raw_paragraph'].apply(lambda paragraph:self.clean_paragraph(paragraph))\n",
        "    print(\"Paragraph Cleaned!\")\n",
        "\n",
        "    cleaned_paragraphs = self.create_paragraphs()\n",
        "    print(\"We have {} paragraphs.\".format(len(cleaned_paragraphs)))\n",
        "    cleaned_paragraphs = np.unique(cleaned_paragraphs)\n",
        "    print(\"Now we have {} unique paragraphs.\".format(len(cleaned_paragraphs)))\n",
        "\n",
        "    self.find_positive_words(cleaned_paragraphs)\n",
        "    print(\"Positive words found. We have {} positive words.\".format(len(self.positive_words)))\n",
        "    self.find_negative_words(cleaned_paragraphs)\n",
        "    print(\"Negative words found. We have {} negative words.\".format(len(self.negative_words)))\n",
        "\n",
        "\n",
        "    length = len(cleaned_paragraphs)\n",
        "    label_1 = [1 for _ in range(length)]\n",
        "    label_0 = [0 for _ in range(length)]\n",
        "\n",
        "    cleaned_paragraphs = list(cleaned_paragraphs)\n",
        "    self.positive_words = list(self.positive_words)\n",
        "    self.negative_words = list(self.negative_words)\n",
        "\n",
        "    cleaned_paragraphs.extend(cleaned_paragraphs)\n",
        "    self.positive_words.extend(self.negative_words)\n",
        "    label_1.extend(label_0)\n",
        "\n",
        "    data = {\n",
        "    'Paragraph': cleaned_paragraphs,\n",
        "    'Target Word': self.positive_words,\n",
        "    'Label': label_1\n",
        "    }\n",
        "    self.df = pd.DataFrame(data)\n",
        "    self.drop_duplicates()\n",
        "\n",
        "  def start(self):\n",
        "    self.load_data()\n",
        "    self.create_dataframe()\n",
        "    self.create()\n",
        "    print(\"Creation is DONE\")\n",
        "    self.save()\n",
        "    print(\"Dataframe is SAVED\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VojFfzQwRISS"
      },
      "outputs": [],
      "source": [
        "dataset = DatasetCreation(glove_model, 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ32smK9RakC"
      },
      "outputs": [],
      "source": [
        "dataset.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "HkjeiO5amMe0",
        "outputId": "733044c0-96b2-42cb-c0a5-a1fa2fd0a40a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Label\n",
              "count  11158.000000\n",
              "mean       0.500000\n",
              "std        0.500022\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.500000\n",
              "75%        1.000000\n",
              "max        1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df57b948-6475-4d64-a5de-5f2f1f4ae0ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11158.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.500022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df57b948-6475-4d64-a5de-5f2f1f4ae0ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df57b948-6475-4d64-a5de-5f2f1f4ae0ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df57b948-6475-4d64-a5de-5f2f1f4ae0ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-56c3b395-6464-4e31-837e-5e7e8e6b2342\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56c3b395-6464-4e31-837e-5e7e8e6b2342')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-56c3b395-6464-4e31-837e-5e7e8e6b2342 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x7d1f2cb771c0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Values</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "import numpy as np\n",
              "from google.colab import autoviz\n",
              "\n",
              "def value_plot(df, y, figscale=1):\n",
              "  from matplotlib import pyplot as plt\n",
              "  df[y].plot(kind='line', figsize=(8 * figscale, 4 * figscale), title=y)\n",
              "  plt.gca().spines[['top', 'right']].set_visible(False)\n",
              "  plt.tight_layout()\n",
              "  return autoviz.MplChart.from_current_mpl_state()\n",
              "\n",
              "chart = value_plot(_df_0, *['Label'], **{})\n",
              "chart"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-2da734e5-bbdd-43ca-bfd5-cd6505c97194\">\n",
              "        <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMQAAABgCAYAAABCK92TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAANiklEQVR4nO3de1ATd7sH8G8IErRA4FCJlDsiCgU2Y4AiY0WxOq+tMr1omc50\n",
              "Olinaue82qltvVSmPWor9kht+/JO37YcxbE9RzserVrq0aEtlYu8r/Ie0V5ouQgST7kURUIChFye\n",
              "8wc23XCTe0LyfGZ2xt397eZZhy/PbjZZJEREYIwBAFxsXQBj9oQDwZgIB4IxEQ4EYyIcCMZEOBCM\n",
              "iXAgGBPhQDAmwoFgTIQD4aDq6+shkUhQU1Mz6n0cPnwYgYGB41iV/eNA2LnFixcjMzPT1mU4DQ4E\n",
              "YyIciCnqhx9+wNKlSzFz5kzI5XI89NBD+Pbbb/uNKywsRHR0NLy8vLB06VLU1tZa1plMJrz77ruI\n",
              "ioqCXC6HSqXCN998M5mHYXc4EFPY9u3b0dDQgJaWFqxYsQJPPPEEWlparMbk5ubi/PnzaGpqQlhY\n",
              "GFatWgWj0QgA2LNnDz799FOcOnUKbW1tyMzMRFpamlVonA4xu5aSkkI7d+4c1li5XE5nzpwhIqK6\n",
              "ujoCYJknItJoNCSVSqmoqIiIiLy8vOjcuXNW+3jkkUdoz549RESUl5dHAQEB43EYU4arrQPJRqeh\n",
              "oQFbt27FxYsXcefOHbi4uECj0fTrEGFhYZZ/e3p64v7774darUZzczM0Gg3WrFkDF5c/ThQMBgMi\n",
              "IiIm7TjsDQdiinrhhRcgl8tx+fJlKBQKEBF8fHxAfb7vVV9fj5iYGACAVqtFa2srAgMD4e3tDXd3\n",
              "d+Tn52PRokW2OAS7xNcQU4DJZEJ3d7fV1NbWBg8PD/j4+ECn02HHjh3QarX9tn3rrbdw8+ZNdHZ2\n",
              "4pVXXkFERASSk5Mhk8mwceNGbN26FZWVlSAidHV1oaioCFVVVTY4SvvAgZgC9u3bh+nTp1tNq1ev\n",
              "xtWrV+Hj44Po6GgEBAQMeBNt3bp1WLZsGRQKBaqqqvDll1/C1bX3xCA7OxvPPPMM1qxZA29vb4SG\n",
              "hiIrKwsGg2GyD9FuSKhvj2XMiXGHYEyEA8GYCAeCMREOBGMiHAjGRDgQjIlwIBgTcehAfPDBB/2W\n",
              "3enswW1djw2qYVOBQwfixo0bVvPHy9VY+E4hPvjaeT+awIY2okBs3rwZoaGhkEgkqKiosCyvrq5G\n",
              "cnIyIiMjkZCQgB9//HFC141WgPd0aPVGHL2sRrOme8z7Yw5oJJ8Vv3DhAqnVagoJCaErV65Yli9Z\n",
              "soTy8vKIiOj48eMUHx8/oeuG6+WXX7aaN5vN9NSHpRSyLZ/+7cwPI94fc3yj+oKQOBDNzc3k6elJ\n",
              "BoOBiHp/6BQKBVVXV0/IupHoGwgioqKqFgrZlk+RO89Ss6ZrNIfPHNiYryHUajX8/f0tn6CUSCQI\n",
              "Dg5GQ0PDhKwbjF6vh0ajsZpMJlO/cQsj7sf8YG/ojWZ8cuH6WA+fORiHuajOysqCXC63mi5dutRv\n",
              "nEQiwealcwAAn/3jBlq1+skuldmxMQciKCgIjY2Nli+uExEaGhoQHBw8IesGs2PHDrS3t1tNiYmJ\n",
              "A45NiZwJIcgb3QYzcou4S7A/jDkQfn5+mD9/Pj777DMAwIkTJxAYGIiIiIgJWTcYmUwGLy8vq0kq\n",
              "lQ44ViKR4KWlvfs6UnYDt7hLsN+N5IJj/fr1FBAQQFKplPz8/Gj27NlERPTzzz9TUlISzZkzh1Qq\n",
              "FV27ds2yzUSsG66BLqp/ZzabaeVfiilkWz7t+5/KEe+bOSaH/sbcli1bcODAgUHXF/zUjBeOlOM+\n",
              "NylKtqXC5z63SayO2SOHuagejUei/BDt7wVdjwkHS+psXQ6zA04dCPE7Tocv1uNOJ3/Gydk5dSAA\n",
              "YHm0AvNmeUKrN+JQab2ty2E25vSBcHH5o0vkldahvct5H8HCOBAAgD89OAtzFZ7o6DbiMHcJp8aB\n",
              "QG+X2HT3vsTBkuvQdHOXcFYciLtWxPgjws8Dmm4jjlyst3U5zEY4EHdJXSTYlNrbJf6jpA5avdHG\n",
              "FTFb4ECIrIx7AOEz78OdTgOOlNXbuhxmAxwIEXGXyC26Dh13CafDgehjVdwDCPWdgbZOAz79+417\n",
              "b8AcCgeiD1epC/6c2ntfIrfoOjp7uEs4Ew7EAB5XPoDgf5mBW7oe/OffB/+WHnM8HIgBuEpd8Ocl\n",
              "vdcSHxddR1dP/6+iMsfEgRjEE/MDEOgzHa1aPf7rEncJZ8GBGMQ0qQv+9W6X+OhCLboN3CWcAQdi\n",
              "CE/ND0SA93T81qHHMe4SToEDMQQ3Vxe8uHg2AOBv3CWcAgfiHtbEB8Jf7o5mjR7Hy9W2LodNMA7E\n",
              "PchcpZYu8eF3tdAbuUs4Mg7EMDwdHwSFlwyN7d3473/etHU5bAJxIIbBfZoUG1PudonCWvQYzTau\n",
              "iE0Uuw/ERDwWfzSeSQzGTE8Z/u9OF07+L3cJR2X3gdiwYQPWr1+PqqoqbNu2DRkZGTapw32aFBsW\n",
              "hQMA/lpYA4OJu4QjsusHlbW0tCAiIgK3b9+Gq6sriAj+/v4oKSkZ8rGWv7vXg8pGqqvHhIf//Vu0\n",
              "anvw1PxAhPjOGLd9T2X29hO0flE4prsN/BjTe3Ed51rG1VCPxe8bCL1eD73e+hmtAz0Ofyymu0mx\n",
              "flE49p79GSf4tMluPbcgxDEDMRJZWVnYtWuX1bKkpKRxf52M5DBo9SZ+jH4fElsXIOLmOvorAYc5\n",
              "ZRqoQzz33HMIDw+3WmYymXDp0iUkJiYO+nTwycB1TG4dISEheOmll+490HbPWR6elJQUq781p1Kp\n",
              "xrS/9vZ2AkDt7e3jUB3X4Wh12P0p08cff4yMjAzs3bsXXl5eyMvLs3VJzIHZfSDmzp2LsrIyW5fB\n",
              "nITd34dgbDI5XSBkMhnefPNNyGQyroPr6Meu32VibLI5XYdgbCgcCMZEOBCMiXAghuH9999HU1PT\n",
              "uOyrvr4eH3300ZBj4uPj8d133436Nbq7u/H4448jMjISgiBg2bJlqKmpAQAsXrwYYWFhUCqVUCqV\n",
              "eO+99yzbbdiwAbGxsUhNTUV7ezsAgIiwYsUK1NbWjqqWW7duWV5LqVQiMjISrq6uuH379qTXMiw2\n",
              "vS04RYSEhNCVK1fGZV+FhYUkCMKQY1QqFRUWFo76Nbq6uuirr74is9lMREQ5OTmUkpJCRL13/r/4\n",
              "4ot+23z//fe0ZMkSIiLatWsX5eTkEBHRJ598Qu+8886oa+lr//79tHLlSruoZSBTukOUlZVh4cKF\n",
              "EAQBcXFxOH36NMrLy5GcnIy4uDgkJiaitLQUQO9vZm9vb8u2Wq0WEskfH0mTSCTYu3cvEhMTERYW\n",
              "Zrkjvnv3bvz6669IT0+HUqlERUXFsOvr6upCeno6oqOjIQgCli9fjo0bN+KXX36BUqlEWloaAODi\n",
              "xYtQKpWIiYnB2rVrYTSO7Xmy7u7uePTRRy3Hl5SUhPr6+iG3mTZtGvR6PcxmM3Q6Hdzc3NDY2Iij\n",
              "R49iy5YtY6pH7ODBg1i3bp1d1DKgCY3bBLp16xb5+flRUVERERGZTCZqbm6moKAgOnfuHBERFRcX\n",
              "k0KhoI6ODqqrqyO5XG7ZvqOjg8SHD4Cys7OJiKiyspI8PDzIYDAQ0eg7xMmTJ2n58uVWNfftEHq9\n",
              "ngIDA6mgoICIiM6fP08AxtQh+nr22Wdp8+bNRNT7W3nu3LkUExNDTz/9NNXW1lrG7dy5kwRBoNWr\n",
              "V5NOp6P09HQqLy8ftzpKS0tJoVBY/l9tWctgpmwg8vPz6eGHH7Zadu3aNQoJCbFaFhcXR8XFxcMK\n",
              "RGNjo2Xe29ub1Go1EY0+ELW1tRQUFEQvvvgiHTt2jDQaTb9AXL16lUJDQ622Cw8PH7dAvP3225SU\n",
              "lEQ6nY6IiBoaGoiIyGw2U05ODkVFRQ243alTp+jVV1+l1tZWysjIoCeffJKOHTs2plqef/55eu21\n",
              "1yzztqxlMA4fCEEQqLi4mNRqNXl4eFiW//bbb/0C0dbWZpn39fWluro6IhrbNURHRwedPn2aNm3a\n",
              "RMHBwXTy5Ml7BmL27NnjEoj9+/eTSqWyOq6+ZDIZtba2Wi1rb2+nhQsXkk6no8zMTMrLyyODwUDR\n",
              "0dHU2dk5qlo6OjrIw8ODKisrbV7LUKbsNURycjKqq6tRXFwMADCbzVAoFDCbzSgoKADQe27e1NQE\n",
              "pVKJWbNmgYjw008/AQCOHDky7Nfy8vKyvNMxEjdv3oREIkFaWhqys7NBRPD19bXa17x582A0GlFY\n",
              "WAgA+Prrr8flXZQDBw7g6NGjKCgosFw7GY1GNDc3W8acOHECCoUCvr6+Vttu374db7zxBmbMmAGd\n",
              "TgeJRAKJRAKDwYCenp5R1fP5559DEATMmzfP5rUMadwjNonKysooOTmZYmNjSRAEOnPmDF2+fJkW\n",
              "LFhAsbGxlJCQQMXFxZbxhw4dorCwMIqPj6d9+/YNu0Pk5ubSnDlzSBCEEXWKs2fPkiAIFBcXR9HR\n",
              "0fT666+TwWCgxx57jB588EFatWoVEfWeWwuCQDExMbR27VoSBGFMHUKtVhMACg8PJ0EQSBAESkxM\n",
              "JK1WSyqVimJiYiguLo5SU1OpoqLCatuSkhLKyMiwzNfU1FBCQgJFRUXR7t27R13TggUL6NChQ5Z5\n",
              "W9YyFP4sE2MiU/aUibGJwIFgTIQDwZgIB4IxEQ4EYyIcCMZEOBCMiXAgGBPhQDAmwoFgTIQDwZjI\n",
              "/wPG0hLJ1KZ1wQAAAABJRU5ErkJggg==\n",
              "\">\n",
              "        \n",
              "      </div>\n",
              "      <script></script>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-2da734e5-bbdd-43ca-bfd5-cd6505c97194\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-2da734e5-bbdd-43ca-bfd5-cd6505c97194\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x7d1f2398be50>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "import numpy as np\n",
              "from google.colab import autoviz\n",
              "\n",
              "def histogram(df, colname, num_bins=20, figscale=1):\n",
              "  from matplotlib import pyplot as plt\n",
              "  df[colname].plot(kind='hist', bins=num_bins, title=colname, figsize=(8*figscale, 4*figscale))\n",
              "  plt.gca().spines[['top', 'right',]].set_visible(False)\n",
              "  plt.tight_layout()\n",
              "  return autoviz.MplChart.from_current_mpl_state()\n",
              "\n",
              "chart = histogram(_df_1, *['Label'], **{})\n",
              "chart"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-09088634-8d09-4178-b479-f853ed0c1420\">\n",
              "        <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAALkAAABgCAYAAABBnNO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAALIklEQVR4nO3de0xT5xsH8G8prmNKWwQkCFqQS73VolXUmuiM8w+3iIozxmVR\n",
              "1AhTozOYMIkmXjCBP9yi/mGM/oGbS9Rt4mUuaiISWeamDJyKFypGLNuUOoXWCyiU5/eHsT9rC/K2\n",
              "pxfq80lOpD0vh+eJD29fzvuec2RERGAshIUFOgDGfI2LnIU8LnIW8rjIWcjjImchj4uchTwuchby\n",
              "uMhZyOMiZyGPizzINTQ0QCaTob6+3uNj7Nu3D4mJiRJG1btwkfvJhx9+iA0bNgQ6jHcSFzkLeVzk\n",
              "AVZbW4tp06YhNjYWKpUK48ePx9mzZ13aVVRUYPjw4VAqlZg2bRpu377t2Ge32/H1119j2LBhUKlU\n",
              "MBgMKC8v92caQY2LPAisW7cOZrMZFosFM2bMwJw5c2CxWJza7N27F6dPn8b9+/eRnJyMmTNnoqOj\n",
              "AwBQVFSE/fv34+jRo2hubsaGDRuQlZXl9IvwTiPmF1OmTKH169f3qK1KpaLjx48TEdGdO3cIgOM1\n",
              "EZHNZiO5XE6VlZVERKRUKunUqVNOx/joo4+oqKiIiIhKS0spISFBijR6pfBA/5K968xmMwoKCnD+\n",
              "/Hm0tLQgLCwMNpvNpSdPTk52fB0ZGYmYmBg0NjaiqakJNpsN8+bNQ1jY/z+Y29vbkZqa6rc8ghkX\n",
              "eYAtW7YMKpUKVVVViIuLAxEhKioK9Ma1LA0NDRg5ciQA4MmTJ/jvv/+QmJgItVqN999/HydOnMDk\n",
              "yZMDkULQ4zG5H9ntdrS1tTltzc3N6NevH6KiovD06VMUFhbiyZMnLt+7detW/P3333j27BnWrl2L\n",
              "1NRUGI1GKBQKfPHFFygoKMCNGzdARGhtbUVlZSVMJlMAsgw+XOR+VFJSgoiICKft008/xeXLlxEV\n",
              "FYXhw4cjISHB7cTN0qVLMX36dMTFxcFkMuHnn39GePjLD+Jt27ZhwYIFmDdvHtRqNZKSklBcXIz2\n",
              "9nZ/pxiUZPTm5yJjIUa4Jz99+rQv4mDMZ4SLfMuWLdBqtdixYwdsNpsvYmJMUsJF/ttvv+HgwYOo\n",
              "ra1Feno6VqxYgevXr/siNsYk4dWY/K+//kJWVhb+/fdfTJ06Fd988w10Op2U8THmNY/Orpw5cwaz\n",
              "Zs1CdnY2Vq5cifv37yMvLw9z5syROj7GvCbckw8bNgwxMTFYvXo1srOzIZfLHftmzJiBkydPSh4k\n",
              "Y94QLvLq6moYDAZfxcOY5ISHK9XV1Xj06JHj9cOHD7F3715JgwKAHTt2SH5M9m4SLvJdu3ahf//+\n",
              "jtfR0dHYtWuXpEEBwN27dyU/Jns3CS/Qcje6sdvtkgTTE0nrfulyX0PJJ36Lg/Uewj15fHw8fvjh\n",
              "B8frQ4cOIT4+XtKgGJOScE++fft2zJo1CwUFBQCADz74AMeOHZM8MMakIlzkQ4cOxfXr11FXVwcA\n",
              "0Gq1TqcRGQs2Hl00IZPJoFar0dHRgX/++QcAMHjwYEkDY0wqwkW+b98+rF69Gn369HFcbiWTyVwu\n",
              "12IsWAgXeVFREaqqqqDVat/aNikpCQqFAhEREQCAwsJCzJ8/XzxKxrwgXOQxMTE9KvBXDh06hIyM\n",
              "DNEfw5hkhE8hzp49G9u3b4fFYoHNZnNsjAUr4Z58/fr1AID8/HzIZDIQEWQyWZcTQgsXLgQRITMz\n",
              "EyUlJYiNjXVp8/z5czx//tzpPX9OMLHQJtyTd3Z2Oja73e74153KykpcuXIFNTU1iImJwaJFi9y2\n",
              "Ky4uhkqlctouXrwoGhpjbnm0nry6uhr79+8HALS0tODevXtu2706rdinTx+sWbMGv/76q9t2hYWF\n",
              "sFqtTltmZqYnoTHmwqMFWkuWLMGmTZsAvFyF+Nlnn7m0e/r0KVpaWhyvDxw4gNGjR7s9pkKhgFKp\n",
              "dNp4golJRXhMvmfPHvzxxx8wGo0AgJSUFDx48MClXVNTE+bOnQu73Q4iwpAhQ/Ddd995HzFjgoSL\n",
              "/PXz3o6DhLseZsiQIbh06ZLnkTEmEeHhSmxsLEwmE2QyGYCXM6A8pc+CmUerEBcsWICbN29i0KBB\n",
              "UCqVOHHihC9iY0wSwkWempqKCxcuoK6uDkTEqxBZ0BMucrPZDADo27cvAPAqRBb0hIvcYDA4Zjrb\n",
              "2trw7NkzREdH8ypEFrSEi/zN04VlZWW4fPmyZAExJjWv70+enZ2NX37p+uJixgJNuCd/fcWh3W7H\n",
              "hQsXeBUiC2rCRa5Wqx1jcrlcjrS0NOzcudMXsTEmCeEi7+zs9EUcjPkMPzOIhTzhnjwsLMwxpf+6\n",
              "t108wVigCBf5li1b0NraiuXLlwMAdu/ejYiICKxZs0bq2BiThHCRHzlyBNXV1Y7XW7duhcFgcFwW\n",
              "x1iwER6TP3782Gl202Kx4PHjx5IGxZiUhHvytWvXQq/X4+OPPwYAnDp1ynGVEGPBSLjI8/LyMGnS\n",
              "JFRUVAB4edX+iBEjJA+MvXu6ui23t7fk9ugUYnR0NHQ6HVatWgWtVosXL164bXfr1i0YjUakp6dj\n",
              "3LhxuHbtmlfBMuYJ4SL/6aefMGHCBCxevBgAcO3aNcyePdtt27y8POTm5sJkMuGrr75CTk6ON7Ey\n",
              "5hHhIi8uLkZNTQ3UajUAQK/Xu330icViwZ9//onPP/8cADB37lw0Njaivr7eu4gZEyQ8JpfL5YiO\n",
              "jnZ677333nNp19jYiPj4eMdFzjKZDIMHD4bZbEZqaqpTW76DFvMl4SKPjIxEU1OTY9azvLzc6UFZ\n",
              "niguLsbmzZud3hs6dCjy8/Od3rPb7Yi/eBGZmZluL7nLzy/3Ko5As9vtuNhNfr3d2/LL7uL7uvp/\n",
              "1Wg0+PLLL9/+g0lQVVUVjR49mlQqFU2aNIkGDhxIly5dcmnX1NREkZGR1N7eTkREnZ2dFBcXR7du\n",
              "3XJp29bWRlar1Wlra2tzaWe1WgkAWa1W0bB7Bc7PN4R68lf3PayoqMD58+dBRDAajY7x+esGDBiA\n",
              "MWPG4Pvvv0dOTg4OHz6MxMREl6EK8PJeLgqFQiQUxnpM+InMer2+x5e71dXVIScnBw8fPoRSqURp\n",
              "aSl0Op1HgQIvL9hQqVSwWq1QKpUeHydYcX6+ITwmT0tLQ319vdse+U1arRa///67R4ExJhXhIn/0\n",
              "6BEyMjJgNBrRr18/x/tlZWWSBuaOQqHAxo0bQ3Zow/n5Ro+HK7m5udizZw++/fZbNDc3Iyoqyml/\n",
              "V/ceZyzQelzkY8aMQU1NjcvXjAU7j9auCP6tylhA9XhM3traiqtXrzrunPXq61dGjRrlkwAZ81pP\n",
              "T6hrNBpKSkpyuyUnJ0t/Bt8Nk8lEEydOpLS0NBo7dizV1tb65ed6Q6PRUHp6Oun1etLr9XTw4EEi\n",
              "6j4XT/f5w6pVq0ij0RAAp0lAX+QjVa7CM56BNHXqVCotLSUioh9//JHGjh0b2IB6QKPRuJ0R7i4X\n",
              "T/f5w7lz56ixsdElL1/kI1WuvabIRZYJBBN3Rd5dLp7u87fX8/JFPlLm2mvuu9LdqsZgt3DhQuh0\n",
              "OixduhQPHjzoNhdP9wWSL/KRMtdeU+S9VU+fZcp8R3jGM1AGDRqEe/fuoaOjA+Hh4SAimM3moL/5\n",
              "/5vPMk1PT+82F6VS6dG+QPJFPlLm2mt68tdXNQLodlVjsOjqWabd5eLpvkDyRT6S5urF3x5+d/Pm\n",
              "TZowYQKlpaWRwWCgK1euBDqkbt2+fZsyMjJIp9PRyJEjKSsri+7cuUNE3efi6T5/yM3NpYSEBJLL\n",
              "5TRgwABKSUnxWT5S5Sq81Jax3qbXDFcY8xQXOQt5XOQs5HGRs5DHRc5CHhc5C3lc5CzkcZGzkMdF\n",
              "zkIeFzkLeVzkLOT9D2Fsx5e6RX4pAAAAAElFTkSuQmCC\n",
              "\">\n",
              "        \n",
              "      </div>\n",
              "      <script></script>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-09088634-8d09-4178-b479-f853ed0c1420\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-09088634-8d09-4178-b479-f853ed0c1420\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset.df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBbaGBp0JQPp"
      },
      "outputs": [],
      "source": [
        "class Statistics():\n",
        "  def __init__(self, filename, glove_model):\n",
        "    self.df = pd.read_csv(filename)\n",
        "    self.path = \"/content/gdrive/MyDrive/Memory Retention Research/Datasets/Dataset_10000/Dataset_10000_temp.csv\"\n",
        "\n",
        "    null_rows = self.df.isnull().any(axis=1)\n",
        "    rows_with_null_data = self.df[null_rows]\n",
        "    paragraph = rows_with_null_data['Paragraph']\n",
        "    self.df = self.df[~self.df['Paragraph'].isin(paragraph)]\n",
        "    self.glove_model = glove_model\n",
        "\n",
        "    with open('bias_lexicon.txt', 'r') as file:\n",
        "      self.bias_words = file.read().splitlines()\n",
        "\n",
        "    self.vad_df = pd.read_csv('NRC-VAD-Lexicon.txt', delimiter='\\t', names=['Words', 'valence', 'arousal', 'dominance'])\n",
        "    self.vad_words = set(self.vad_df['Words'].values)\n",
        "\n",
        "  def calculate_C(self):\n",
        "    C = []\n",
        "    paragraphs = self.df['Paragraph'].values\n",
        "    for paragraph in paragraphs:\n",
        "      tokens = paragraph.split()\n",
        "      C.append(len(tokens))\n",
        "    return C\n",
        "\n",
        "  def calculate_d(self):\n",
        "    d = []\n",
        "    paragraphs = self.df['Paragraph'].values\n",
        "    target_words = self.df['Target Word'].values\n",
        "    for i, paragraph in enumerate(paragraphs):\n",
        "      tokens = paragraph.split()\n",
        "      length = len(tokens)\n",
        "      if target_words[i] in tokens:\n",
        "        ind = tokens.index(target_words[i])\n",
        "        diff = length - ind - 1\n",
        "      else:\n",
        "        diff = -1\n",
        "      d.append(diff)\n",
        "    return d\n",
        "\n",
        "  def calculate_most_similar_and_similarity(self):\n",
        "    sim_words = []\n",
        "    sim_scores = []\n",
        "    target_words = self.df['Target Word'].values\n",
        "    for target_word in tqdm(target_words):\n",
        "      if target_word in self.glove_model.key_to_index:\n",
        "        sim_word, sim_score = self.glove_model.most_similar(target_word)[0]\n",
        "        sim_words.append(sim_word)\n",
        "        sim_scores.append(sim_score)\n",
        "      else:\n",
        "        sim_words.append(-1)\n",
        "        sim_scores.append(-1)\n",
        "\n",
        "    assert len(target_words) == len(sim_words)\n",
        "\n",
        "    return sim_words, sim_scores\n",
        "\n",
        "  def calculate_repetition(self):\n",
        "    repetition = []\n",
        "    paragraphs = self.df['Paragraph'].values\n",
        "    target_words = self.df['Target Word'].values\n",
        "\n",
        "    for i, paragraph in enumerate(paragraphs):\n",
        "      repeat = paragraph.split().count(target_words[i])\n",
        "      repetition.append(repeat)\n",
        "    return repetition\n",
        "\n",
        "  def is_vad(self):\n",
        "    is_vad = []\n",
        "\n",
        "    target_words = self.df['Target Word'].values\n",
        "    for target_word in target_words:\n",
        "      if target_word in self.vad_words:\n",
        "        is_vad.append(1)\n",
        "      else:\n",
        "        is_vad.append(0)\n",
        "\n",
        "    return is_vad\n",
        "\n",
        "  def is_bias(self):\n",
        "    is_bias = []\n",
        "\n",
        "    target_words = self.df['Target Word'].values\n",
        "    for target_word in target_words:\n",
        "      if target_word in self.bias_words:\n",
        "        is_bias.append(1)\n",
        "      else:\n",
        "        is_bias.append(0)\n",
        "\n",
        "    return is_bias\n",
        "\n",
        "  def calculate_POS(self):\n",
        "    poses = []\n",
        "\n",
        "    paragraphs = self.df[self.df['Label']==1]['Paragraph'].values\n",
        "    target_words = self.df[self.df['Label']==1]['Target Word'].values\n",
        "\n",
        "    for i, paragraph in tqdm(enumerate(paragraphs)):\n",
        "      words = nltk.word_tokenize(paragraph)\n",
        "      pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "      for word, pos in pos_tags:\n",
        "        if word == target_words[i]:\n",
        "          poses.append(pos)\n",
        "          break\n",
        "\n",
        "    neg_poses = [-1 for _ in range(len(poses))]\n",
        "    poses.extend(neg_poses)\n",
        "    return poses\n",
        "\n",
        "  def calculate_unigram_frequency(self):\n",
        "    word_freq = []\n",
        "\n",
        "    paragraphs = np.unique(self.df['Paragraph'].values)\n",
        "    paragraphs = list(paragraphs)\n",
        "    document = ' '.join(paragraphs)\n",
        "\n",
        "    words = document.split()\n",
        "    word_frequency = Counter(words)\n",
        "\n",
        "    target_words = self.df['Target Word'].values\n",
        "    for word in target_words:\n",
        "      if word in word_frequency:\n",
        "        word_freq.append(word_frequency[word])\n",
        "      else:\n",
        "        word_freq.append(0)\n",
        "    return word_freq\n",
        "\n",
        "  def calculate_query_length(self):\n",
        "    query_length = []\n",
        "    words = self.df['Target Word'].values\n",
        "    for query in words:\n",
        "      query_length.append(len(query))\n",
        "    return query_length\n",
        "\n",
        "  def save(self, filename, base_path):\n",
        "    self.df.to_csv(filename, index=False)\n",
        "\n",
        "    pos_df = self.df[self.df['Label']==1]\n",
        "    neg_df = self.df[self.df['Label']==0]\n",
        "\n",
        "    pos_train_df, pos_remaining = train_test_split(pos_df, test_size=0.1, random_state=42)\n",
        "    pos_valid_df, pos_test_df = train_test_split(pos_remaining, test_size=0.5, random_state=42)\n",
        "\n",
        "    neg_train_df, neg_remaining = train_test_split(neg_df, test_size=0.1, random_state=42)\n",
        "    neg_valid_df, neg_test_df = train_test_split(neg_remaining, test_size=0.5, random_state=42)\n",
        "\n",
        "    train_df = pd.concat([pos_train_df, neg_train_df])\n",
        "    test_df = pd.concat([pos_test_df, neg_test_df])\n",
        "    val_df = pd.concat([pos_valid_df, neg_valid_df])\n",
        "\n",
        "    train_df.to_csv( base_path + \"Dataset_10000/train.csv\", index=False)\n",
        "    test_df.to_csv(base_path + \"Dataset_10000/test.csv\", index=False)\n",
        "    val_df.to_csv(base_path + \"Dataset_10000/val.csv\", index=False)\n",
        "\n",
        "  def save_periodically(self):\n",
        "    self.df.to_csv(self.path, index=False)\n",
        "\n",
        "  def run_statistics(self):\n",
        "    self.df['C'] = self.calculate_C()\n",
        "    self.save_periodically()\n",
        "    print(\"C is calculated and saved\")\n",
        "\n",
        "    self.df['d'] = self.calculate_d()\n",
        "    self.save_periodically()\n",
        "    print(\"d is calculated and saved\")\n",
        "\n",
        "    self.df['most_similar'], self.df['similarity_score'] = self.calculate_most_similar_and_similarity()\n",
        "    self.save_periodically()\n",
        "    print(\"most similar and similarity is calculated and saved\")\n",
        "\n",
        "    self.df['POS'] = self.calculate_POS()\n",
        "    self.save_periodically()\n",
        "    print(\"POS is calculated and saved\")\n",
        "\n",
        "    self.df['repetitions'] = self.calculate_repetition()\n",
        "    self.save_periodically()\n",
        "    print(\"repetitions is calculated and saved\")\n",
        "\n",
        "    self.df['is_vad'] = self.is_vad()\n",
        "    self.save_periodically()\n",
        "    print(\"vad is calculated and saved\")\n",
        "\n",
        "    self.df['is_bias'] = self.is_bias()\n",
        "    self.save_periodically()\n",
        "    print(\"bias is calculated and saved\")\n",
        "\n",
        "    self.df['unigram_frequency'] = self.calculate_unigram_frequency()\n",
        "    self.save_periodically()\n",
        "    print(\"unigram_frequency is calculated and saved\")\n",
        "\n",
        "    self.df['query_length'] = self.calculate_query_length()\n",
        "    self.save_periodically()\n",
        "    print(\"length of query is calculated and saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbKbm5MFt2fS",
        "outputId": "91afe3e6-7934-4c7f-c23a-9c30332faf5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Memory Retention Research/Datasets/Dataset_10000/Dataset_10000.csv\n"
          ]
        }
      ],
      "source": [
        "print(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiACLwugz03z"
      },
      "outputs": [],
      "source": [
        "statistics = Statistics(dataset_path, glove_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRGpAD5jz9Zn",
        "outputId": "8b7b313c-81ea-42e0-e6ba-78a8fb873749"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C is calculated and saved\n",
            "d is calculated and saved\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 11158/11158 [12:38<00:00, 14.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "most similar and similarity is calculated and saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5579it [55:20,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS is calculated and saved\n",
            "repetitions is calculated and saved\n",
            "vad is calculated and saved\n",
            "bias is calculated and saved\n",
            "unigram_frequency is calculated and saved\n",
            "length of query is calculated and saved\n"
          ]
        }
      ],
      "source": [
        "statistics.run_statistics()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "statistics.save(dataset_path, base_path)"
      ],
      "metadata": {
        "id": "D3xwakqR4Qbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNLjAAHtnOtG"
      },
      "outputs": [],
      "source": [
        "class Embedding():\n",
        "  def __init__(self, models, tokenizers):\n",
        "    # self.glove_model, self.bert_model, self.roberta_model, self.gpt_model, self.T5_model = models\n",
        "    self.glove_model = models\n",
        "    # self.bert_tokenizer, self.roberta_tokenizer, self.gpt_tokenizer, self.T5_tokenizer = tokenizers\n",
        "\n",
        "    self.VECTOR_SIZE = 300\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def glove_representation(self, text):\n",
        "    tokens = text.split()\n",
        "    doc_vec = np.zeros((len(tokens), 300))\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in glove_model.key_to_index:\n",
        "          doc_vec[i] = glove_model[token]\n",
        "    doc_vec = np.mean(doc_vec, axis=0, keepdims=True)\n",
        "    return doc_vec\n",
        "\n",
        "  def bert_representation(self, text):\n",
        "    chunk_size=500\n",
        "    tokens = text.split()\n",
        "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "    doc_vec = []\n",
        "    for chunk in chunks:\n",
        "      input_ids = torch.tensor(self.bert_tokenizer.encode(chunk)).unsqueeze(0).to(self.device)\n",
        "      outputs = self.bert_model(input_ids)\n",
        "      last_hidden_states = outputs[0]\n",
        "      cls_head = last_hidden_states[:, 0, :]\n",
        "      doc_vec.append(cls_head)\n",
        "    concatenated_doc_vec = torch.cat(doc_vec, dim=0)\n",
        "    mean_doc_vec = torch.mean(concatenated_doc_vec, dim=0)\n",
        "    return mean_doc_vec.detach().cpu().numpy().reshape(1,-1)\n",
        "\n",
        "  def roberta_representation(self, text):\n",
        "    chunk_size=500\n",
        "    tokens = text.split()\n",
        "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "    doc_vec = []\n",
        "    for chunk in chunks:\n",
        "      input_ids = torch.tensor(self.roberta_tokenizer.encode(chunk)).unsqueeze(0).to(self.device)\n",
        "      outputs = self.roberta_model(input_ids)\n",
        "      last_hidden_states = outputs[0]\n",
        "      cls_head = last_hidden_states[:, 0, :]\n",
        "      doc_vec.append(cls_head)\n",
        "    concatenated_doc_vec = torch.cat(doc_vec, dim=0)\n",
        "    mean_doc_vec = torch.mean(concatenated_doc_vec, dim=0)\n",
        "    return mean_doc_vec.detach().cpu().numpy().reshape(1,-1)\n",
        "\n",
        "  def gpt_representation(self, text):\n",
        "    tokens = self.gpt_tokenizer.tokenize(text)\n",
        "    token_ids = self.gpt_tokenizer.convert_tokens_to_ids(tokens)\n",
        "    pretrained_embeddings = self.gpt_model.transformer.wte.weight\n",
        "    embeddings = pretrained_embeddings[token_ids]\n",
        "    sentence_embedding = torch.mean(embeddings, dim=0)\n",
        "    return sentence_embedding.cpu().detach().numpy()\n",
        "\n",
        "  def T5_representation(self, text):\n",
        "    chunk_size=500\n",
        "    tokens = text.split()\n",
        "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "    doc_vec = []\n",
        "    for chunk in chunks:\n",
        "      input_ids = torch.tensor(self.T5_tokenizer.encode(chunk)).unsqueeze(0).to(self.device)\n",
        "      outputs = self.T5_model.encoder(input_ids=input_ids)\n",
        "      last_hidden_states = outputs.last_hidden_state\n",
        "      cls_head = last_hidden_states[:, 0, :]\n",
        "      doc_vec.append(cls_head)\n",
        "    concatenated_doc_vec = torch.cat(doc_vec, dim=0)\n",
        "    mean_doc_vec = torch.mean(concatenated_doc_vec, dim=0)\n",
        "    return mean_doc_vec.detach().cpu().numpy().reshape(1, -1)\n",
        "\n",
        "  def embed(self, df, technique='glove'):\n",
        "    if technique=='glove':\n",
        "      df['paragraph_representation'] = df['Paragraph'].apply(lambda text:self.glove_representation(text))\n",
        "      print(\"Paragraph Representation Done.\")\n",
        "\n",
        "      df['word_representation'] = df['Target Word'].apply(lambda word:self.glove_representation(word))\n",
        "      print(\"Word Representation Done.\")\n",
        "\n",
        "    elif technique=='bert':\n",
        "      df['paragraph_representation'] = df['Paragraph'].apply(lambda text:self.bert_representation(text))\n",
        "      print(\"Paragraph Representation Done.\")\n",
        "\n",
        "      df['word_representation'] = df['Target Word'].apply(lambda word:self.bert_representation(word))\n",
        "      print(\"Word Representation Done.\")\n",
        "\n",
        "    elif technique=='roberta':\n",
        "      self.df['paragraph_representation'] = self.df['Paragraph'].apply(lambda text:self.roberta_representation(text))\n",
        "      print(\"Paragraph Representation Done.\")\n",
        "\n",
        "      self.df['word_representation'] = self.df['Target Word'].apply(lambda word:self.roberta_representation(word))\n",
        "      print(\"Word Representation Done.\")\n",
        "\n",
        "    elif technique=='gpt':\n",
        "      self.df['paragraph_representation'] = self.df['Paragraph'].apply(lambda text:self.gpt_representation(text))\n",
        "      print(\"Paragraph Representation Done.\")\n",
        "\n",
        "      self.df['word_representation'] = self.df['Target Word'].apply(lambda word:self.gpt_representation(word))\n",
        "      print(\"Word Representation Done.\")\n",
        "\n",
        "    elif technique=='T5':\n",
        "      self.df['paragraph_representation'] = self.df['Paragraph'].apply(lambda text:self.T5_representation(text))\n",
        "      print(\"Paragraph Representation Done.\")\n",
        "\n",
        "      self.df['word_representation'] = self.df['Target Word'].apply(lambda word:self.T5_representation(word))\n",
        "      print(\"Word Representation Done.\")\n",
        "\n",
        "    elif technique=='lstm':\n",
        "      with open('paragraphs.txt', 'w') as file:\n",
        "        for paragraph in self.df['Paragraph']:\n",
        "          file.write(paragraph + '\\n')\n",
        "\n",
        "      !bash LASER/tasks/embed/embed.sh ./paragraphs.txt ./paragraph_rep.bin\n",
        "\n",
        "      dim = 1024\n",
        "      X = np.fromfile(\"./paragraph_rep.bin\", dtype=np.float32, count=-1)\n",
        "      X.resize(X.shape[0] // dim, dim)\n",
        "      self.df['paragraph_representation'] = [X[i].reshape(1, -1) for i in range(X.shape[0])]\n",
        "\n",
        "      self.merge_positive_and_negative()\n",
        "      print(\"Merge Done.\")\n",
        "\n",
        "      with open('target_words.txt', 'w') as file:\n",
        "        for paragraph in self.df['target_word']:\n",
        "          file.write(paragraph + '\\n')\n",
        "\n",
        "      !bash LASER/tasks/embed/embed.sh ./target_words.txt ./word_rep.bin\n",
        "\n",
        "      X = np.fromfile(\"./word_rep.bin\", dtype=np.float32, count=-1)\n",
        "      X.resize(X.shape[0] // dim, dim)\n",
        "      self.df['word_representation'] = [X[i].reshape(1, -1) for i in range(X.shape[0])]\n",
        "\n",
        "    return df\n",
        "\n",
        "  def run(self, dfs, technique='glove'):\n",
        "    self.train = self.embed(dfs['train'], technique)\n",
        "    self.test = self.embed(dfs['test'], technique)\n",
        "    self.val = self.embed(dfs['val'], technique)\n",
        "    print(\"Everything DONE!\")\n",
        "    print()\n",
        "\n",
        "  def get_inputs(self, df, embed_dim):\n",
        "\n",
        "    return self.train, self.test, self.val\n",
        "\n",
        "    # X1 = np.array(df['paragraph_representation'].values.tolist()).reshape(-1, embed_dim)\n",
        "    # X2 = np.array(df['word_representation'].values.tolist()).reshape(-1, embed_dim)\n",
        "    # y = np.array(df['Label'].values.tolist()).reshape(-1, 1)\n",
        "\n",
        "    # return (X1, X2, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1MHCi-LYt6G"
      },
      "outputs": [],
      "source": [
        "data = Embedding(glove_model, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69o4XOAM4MEJ",
        "outputId": "c5cf41ae-61ff-4f2f-dc9e-ab272dd6ae7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paragraph Representation Done.\n",
            "Word Representation Done.\n",
            "Paragraph Representation Done.\n",
            "Word Representation Done.\n",
            "Paragraph Representation Done.\n",
            "Word Representation Done.\n",
            "Everything DONE!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.run(dfs, technique='glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EABcC0YcXRqp"
      },
      "outputs": [],
      "source": [
        "# train = data.get_inputs(data.train, 300)\n",
        "# test = data.get_inputs(data.test, 300)\n",
        "# val = data.get_inputs(data.val, 300)\n",
        "\n",
        "train, test, val = data.get_inputs(data.train, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YrxFaRmbOCn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhnxZPHSbIzO"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, df, transform=None):\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "\n",
        "    self.paragraph_rep = self.df['paragraph_representation']\n",
        "    self.word_rep = self.df['word_representation']\n",
        "    self.label = self.df['Label']\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    paragraph_rep = torch.tensor(self.paragraph_rep[index])\n",
        "    word_rep = torch.tensor(self.word_rep[index])\n",
        "    label = torch.tensor(self.label[index])\n",
        "\n",
        "\n",
        "    return paragraph_rep, word_rep, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2fvr3vQ3L91"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming you have a pandas DataFrame containing your data, e.g., 'data_df'\n",
        "input1_data = torch.tensor(train['paragraph_representation'])\n",
        "input2_data = torch.tensor(train['word_representation'])\n",
        "label_data = torch.tensor(train['Label'])\n",
        "\n",
        "input1_data = torch.reshape(input1_data, (-1, 300))\n",
        "input2_data = torch.reshape(input2_data, (-1, 300))\n",
        "label_data = torch.reshape(label_data, (-1, 1))\n",
        "\n",
        "# Create a TensorDataset from your input data\n",
        "dataset = TensorDataset(input1_data, input2_data, label_data)\n",
        "\n",
        "# Create a DataLoader to iterate over the data in batches\n",
        "batch_size = 64  # Adjust the batch size as per your requirements\n",
        "training_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Now you can use the for loop as you initially intended\n",
        "for batch_idx, (input1, input2, label) in enumerate(training_loader):\n",
        "    # Your training code here\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrHdjzyZgg_H"
      },
      "outputs": [],
      "source": [
        "trainDataset = CustomDataset(train)\n",
        "testDataset = CustomDataset(test)\n",
        "valDataset = CustomDataset(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuARJbnhgf_1"
      },
      "outputs": [],
      "source": [
        "training_loader = DataLoader(trainDataset, batch_size=25, shuffle=False)\n",
        "testing_loader = DataLoader(testDataset, batch_size=25, shuffle=False)\n",
        "validation_loader = DataLoader(valDataset, batch_size=25, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJTRBs8080RD"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "learning_rate = 0.1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGq2IkFgewWZ"
      },
      "outputs": [],
      "source": [
        "class MemoryModel(nn.Module):\n",
        "    def __init__(self, input1_dim, input2_dim, hidden_dim, output_dim, dropout_rate):\n",
        "        super(MemoryModel, self).__init__()\n",
        "        total_input_dim = input1_dim + input2_dim\n",
        "\n",
        "        self.dense = nn.Linear(total_input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        input1 = input1.to(torch.float32)\n",
        "        input2 = input2.to(torch.float32)\n",
        "\n",
        "        inputs = torch.cat((input1, input2), dim=1)\n",
        "        inputs_dot = torch.tensor([torch.dot(torch.flatten(input1), torch.flatten(input2))])\n",
        "\n",
        "        out = self.dense(inputs)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = torch.cat((torch.flatten(out), inputs_dot), dim=0)\n",
        "\n",
        "        output = self.output_layer(out)\n",
        "        output = self.sigmoid(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "I4H0ItheisSY",
        "outputId": "1c618777-10be-40c0-9beb-6489dbdea0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MemoryModel(\n",
            "  (dense): Linear(in_features=600, out_features=300, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (output_layer): Linear(in_features=300, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-89fa5e3b10d6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-72bee085ed21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input1, input2)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_dot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ],
      "source": [
        "model = MemoryModel(300, 300, 300, 1, 0.3)\n",
        "print(model)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch_idx, (input1, input2, label) in enumerate(training_loader):\n",
        "\n",
        "    scores = model(input1, input2)\n",
        "    loss = criterion(scores, torch.flatten(label))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88qos0ID-Pzj"
      },
      "outputs": [],
      "source": [
        "hyperparameters_list = []\n",
        "optimizers = ['Adam']\n",
        "activations = ['relu']\n",
        "units = [300]\n",
        "dropout_rates = [0.1, 0.3, 0.5, 0.8]\n",
        "for optimizer in optimizers:\n",
        "      for activation in activations:\n",
        "        for unit in units:\n",
        "          for dropout_rate in dropout_rates:\n",
        "            hyperparameters_list.append((optimizer, activation, unit, dropout_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7GXYoXpx6qg"
      },
      "outputs": [],
      "source": [
        "class Classifier():\n",
        "\n",
        "  def build_model(self, shape=(300), optimizer='AdaGrad', activation='relu', units=300, dropout_rate=0.5, learning_rate=0.01, no_layers=1):\n",
        "    input_1 = Input(shape=shape)\n",
        "    input_2 = Input(shape=shape)\n",
        "    inputs = Concatenate(axis=1)([input_1, input_2])\n",
        "    dense_layer = Dense(units=units, activation=activation)(inputs)\n",
        "\n",
        "    if no_layers > 1:\n",
        "      for i in range(no_layers-1):\n",
        "        dense_layer = Dense(units=units, activation=activation)(dense_layer)\n",
        "    dropout_layer = Dropout(dropout_rate)(dense_layer)\n",
        "    dot_layer = Dot(axes=1)([input_1, input_2])\n",
        "    concat_layer = Concatenate(axis=1)([dropout_layer, dot_layer])\n",
        "    outputs = Dense(units=1, activation='sigmoid')(concat_layer)\n",
        "\n",
        "    self.model = Model(inputs=[input_1, input_2], outputs=outputs)\n",
        "    if optimizer == 'Adam':\n",
        "      self.model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    elif optimizer == 'AdaGrad':\n",
        "      self.model.compile(optimizer=Adagrad(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    elif optimizer == 'AdaDelta':\n",
        "      self.model.compile(optimizer=Adadelta(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  def print_model_summary(self):\n",
        "    print(self.model.summary())\n",
        "\n",
        "  def train(self, train, val, epochs, batch_size, hyperparameters, shape):\n",
        "    optimizer, activation, units, dropout_rate = hyperparameters\n",
        "    self.build_model(shape, optimizer, activation, units, dropout_rate)\n",
        "\n",
        "    X_train_1, X_train_2, y_train = train\n",
        "    X_val_1, X_val_2, y_val = val\n",
        "\n",
        "    self.history = self.model.fit([X_train_1, X_train_2], y_train, validation_data=([X_val_1, X_val_2], y_val), epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "  def get_model(self):\n",
        "    return self.model\n",
        "\n",
        "  def get_history(self):\n",
        "    return self.history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzD8fsm4Eaop",
        "outputId": "fdb5c87e-257e-4da2-b611-b37e86221f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 600)          0           ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 300)          180300      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 300)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 1)            0           ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 301)          0           ['dropout[0][0]',                \n",
            "                                                                  'dot[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            302         ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 180,602\n",
            "Trainable params: 180,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "classifier = Classifier()\n",
        "classifier.build_model()\n",
        "classifier.print_model_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEUob7tag74V",
        "outputId": "f3e3bd44-d457-419f-afa6-d394c055b255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50 and Batch Size 32\n",
            "Optimizer Adam Activation relu Units 300 Dropout Rate 0.1\n",
            "\n",
            "Epoch 1/50\n",
            "526/526 [==============================] - 3s 4ms/step - loss: 0.5116 - accuracy: 0.7500 - val_loss: 0.4629 - val_accuracy: 0.7859\n",
            "Epoch 2/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.4450 - accuracy: 0.7967 - val_loss: 0.4366 - val_accuracy: 0.8062\n",
            "Epoch 3/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3983 - accuracy: 0.8216 - val_loss: 0.4222 - val_accuracy: 0.8051\n",
            "Epoch 4/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3473 - accuracy: 0.8519 - val_loss: 0.4133 - val_accuracy: 0.8062\n",
            "Epoch 5/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3015 - accuracy: 0.8775 - val_loss: 0.4249 - val_accuracy: 0.8158\n",
            "Epoch 6/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2517 - accuracy: 0.9041 - val_loss: 0.4424 - val_accuracy: 0.8126\n",
            "Epoch 7/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2054 - accuracy: 0.9253 - val_loss: 0.4515 - val_accuracy: 0.8148\n",
            "Epoch 8/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.1672 - accuracy: 0.9430 - val_loss: 0.4959 - val_accuracy: 0.8137\n",
            "Epoch 9/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1307 - accuracy: 0.9581 - val_loss: 0.5122 - val_accuracy: 0.8180\n",
            "Epoch 10/50\n",
            "526/526 [==============================] - 4s 8ms/step - loss: 0.1072 - accuracy: 0.9695 - val_loss: 0.5651 - val_accuracy: 0.8094\n",
            "Epoch 11/50\n",
            "526/526 [==============================] - 5s 10ms/step - loss: 0.0888 - accuracy: 0.9749 - val_loss: 0.5772 - val_accuracy: 0.8126\n",
            "Epoch 12/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.0734 - accuracy: 0.9807 - val_loss: 0.6010 - val_accuracy: 0.8116\n",
            "Epoch 13/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0582 - accuracy: 0.9867 - val_loss: 0.6574 - val_accuracy: 0.7955\n",
            "Epoch 14/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0546 - accuracy: 0.9854 - val_loss: 0.6453 - val_accuracy: 0.8019\n",
            "Epoch 15/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.0455 - accuracy: 0.9889 - val_loss: 0.7379 - val_accuracy: 0.8019\n",
            "Epoch 16/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0401 - accuracy: 0.9902 - val_loss: 0.7329 - val_accuracy: 0.8116\n",
            "Epoch 17/50\n",
            "526/526 [==============================] - 6s 12ms/step - loss: 0.0407 - accuracy: 0.9897 - val_loss: 0.7637 - val_accuracy: 0.8051\n",
            "Epoch 18/50\n",
            "526/526 [==============================] - 4s 8ms/step - loss: 0.0352 - accuracy: 0.9918 - val_loss: 0.7952 - val_accuracy: 0.8030\n",
            "Epoch 19/50\n",
            "526/526 [==============================] - 4s 8ms/step - loss: 0.0349 - accuracy: 0.9908 - val_loss: 0.8586 - val_accuracy: 0.8041\n",
            "Epoch 20/50\n",
            "526/526 [==============================] - 4s 7ms/step - loss: 0.0333 - accuracy: 0.9909 - val_loss: 0.8616 - val_accuracy: 0.7944\n",
            "Epoch 21/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0281 - accuracy: 0.9923 - val_loss: 0.8900 - val_accuracy: 0.7987\n",
            "Epoch 22/50\n",
            "526/526 [==============================] - 2s 5ms/step - loss: 0.0246 - accuracy: 0.9936 - val_loss: 0.8842 - val_accuracy: 0.8019\n",
            "Epoch 23/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0269 - accuracy: 0.9931 - val_loss: 0.9981 - val_accuracy: 0.7966\n",
            "Epoch 24/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0248 - accuracy: 0.9941 - val_loss: 0.9127 - val_accuracy: 0.8180\n",
            "Epoch 25/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0228 - accuracy: 0.9932 - val_loss: 0.9621 - val_accuracy: 0.8019\n",
            "Epoch 26/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0270 - accuracy: 0.9911 - val_loss: 0.9316 - val_accuracy: 0.8094\n",
            "Epoch 27/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.0253 - accuracy: 0.9927 - val_loss: 0.9566 - val_accuracy: 0.8084\n",
            "Epoch 28/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.9714 - val_accuracy: 0.8148\n",
            "Epoch 29/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0149 - accuracy: 0.9963 - val_loss: 1.0067 - val_accuracy: 0.8073\n",
            "Epoch 30/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0219 - accuracy: 0.9935 - val_loss: 1.0315 - val_accuracy: 0.8041\n",
            "Epoch 31/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 1.0218 - val_accuracy: 0.7987\n",
            "Epoch 32/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0200 - accuracy: 0.9940 - val_loss: 1.0531 - val_accuracy: 0.8084\n",
            "Epoch 33/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0176 - accuracy: 0.9947 - val_loss: 1.0773 - val_accuracy: 0.8105\n",
            "Epoch 34/50\n",
            "526/526 [==============================] - 2s 5ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 1.0956 - val_accuracy: 0.8030\n",
            "Epoch 35/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0199 - accuracy: 0.9937 - val_loss: 1.0999 - val_accuracy: 0.7934\n",
            "Epoch 36/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 1.0968 - val_accuracy: 0.7998\n",
            "Epoch 37/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 1.1060 - val_accuracy: 0.8041\n",
            "Epoch 38/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 1.1317 - val_accuracy: 0.8051\n",
            "Epoch 39/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 1.1444 - val_accuracy: 0.8019\n",
            "Epoch 40/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0190 - accuracy: 0.9938 - val_loss: 1.1300 - val_accuracy: 0.7966\n",
            "Epoch 41/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0147 - accuracy: 0.9959 - val_loss: 1.2360 - val_accuracy: 0.8094\n",
            "Epoch 42/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 1.1919 - val_accuracy: 0.8062\n",
            "Epoch 43/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 1.1988 - val_accuracy: 0.8073\n",
            "Epoch 44/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0174 - accuracy: 0.9947 - val_loss: 1.2186 - val_accuracy: 0.7998\n",
            "Epoch 45/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0137 - accuracy: 0.9963 - val_loss: 1.2553 - val_accuracy: 0.8051\n",
            "Epoch 46/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0147 - accuracy: 0.9956 - val_loss: 1.2698 - val_accuracy: 0.7944\n",
            "Epoch 47/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0175 - accuracy: 0.9942 - val_loss: 1.2209 - val_accuracy: 0.8073\n",
            "Epoch 48/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0097 - accuracy: 0.9973 - val_loss: 1.2440 - val_accuracy: 0.8158\n",
            "Epoch 49/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0155 - accuracy: 0.9955 - val_loss: 1.1645 - val_accuracy: 0.8041\n",
            "Epoch 50/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 1.2540 - val_accuracy: 0.8073\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.8519 - accuracy: 0.7580\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 50 and Batch Size 32\n",
            "Optimizer Adam Activation relu Units 300 Dropout Rate 0.3\n",
            "\n",
            "Epoch 1/50\n",
            "526/526 [==============================] - 4s 6ms/step - loss: 0.5188 - accuracy: 0.7454 - val_loss: 0.4670 - val_accuracy: 0.7816\n",
            "Epoch 2/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4559 - accuracy: 0.7871 - val_loss: 0.4400 - val_accuracy: 0.8019\n",
            "Epoch 3/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4192 - accuracy: 0.8081 - val_loss: 0.4271 - val_accuracy: 0.8041\n",
            "Epoch 4/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3848 - accuracy: 0.8320 - val_loss: 0.4221 - val_accuracy: 0.8019\n",
            "Epoch 5/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3469 - accuracy: 0.8511 - val_loss: 0.4145 - val_accuracy: 0.8105\n",
            "Epoch 6/50\n",
            "526/526 [==============================] - 2s 5ms/step - loss: 0.3126 - accuracy: 0.8682 - val_loss: 0.4298 - val_accuracy: 0.8126\n",
            "Epoch 7/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2767 - accuracy: 0.8877 - val_loss: 0.4319 - val_accuracy: 0.8105\n",
            "Epoch 8/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2454 - accuracy: 0.9035 - val_loss: 0.4327 - val_accuracy: 0.8062\n",
            "Epoch 9/50\n",
            "526/526 [==============================] - 2s 5ms/step - loss: 0.2154 - accuracy: 0.9168 - val_loss: 0.4506 - val_accuracy: 0.8180\n",
            "Epoch 10/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1909 - accuracy: 0.9278 - val_loss: 0.4725 - val_accuracy: 0.8233\n",
            "Epoch 11/50\n",
            "526/526 [==============================] - 4s 9ms/step - loss: 0.1729 - accuracy: 0.9356 - val_loss: 0.4951 - val_accuracy: 0.8126\n",
            "Epoch 12/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1486 - accuracy: 0.9477 - val_loss: 0.4978 - val_accuracy: 0.8158\n",
            "Epoch 13/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1405 - accuracy: 0.9483 - val_loss: 0.5166 - val_accuracy: 0.8030\n",
            "Epoch 14/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1246 - accuracy: 0.9566 - val_loss: 0.5498 - val_accuracy: 0.8180\n",
            "Epoch 15/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.1136 - accuracy: 0.9598 - val_loss: 0.5465 - val_accuracy: 0.8041\n",
            "Epoch 16/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1046 - accuracy: 0.9646 - val_loss: 0.5645 - val_accuracy: 0.8191\n",
            "Epoch 17/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0957 - accuracy: 0.9678 - val_loss: 0.5835 - val_accuracy: 0.8116\n",
            "Epoch 18/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0910 - accuracy: 0.9685 - val_loss: 0.6405 - val_accuracy: 0.8116\n",
            "Epoch 19/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0834 - accuracy: 0.9717 - val_loss: 0.6516 - val_accuracy: 0.8062\n",
            "Epoch 20/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.0826 - accuracy: 0.9719 - val_loss: 0.6690 - val_accuracy: 0.8158\n",
            "Epoch 21/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0760 - accuracy: 0.9729 - val_loss: 0.6638 - val_accuracy: 0.8158\n",
            "Epoch 22/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0715 - accuracy: 0.9769 - val_loss: 0.6901 - val_accuracy: 0.8041\n",
            "Epoch 23/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0676 - accuracy: 0.9779 - val_loss: 0.6821 - val_accuracy: 0.8169\n",
            "Epoch 24/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0672 - accuracy: 0.9772 - val_loss: 0.7484 - val_accuracy: 0.8105\n",
            "Epoch 25/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0657 - accuracy: 0.9780 - val_loss: 0.7560 - val_accuracy: 0.8126\n",
            "Epoch 26/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0654 - accuracy: 0.9776 - val_loss: 0.7350 - val_accuracy: 0.8212\n",
            "Epoch 27/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0580 - accuracy: 0.9801 - val_loss: 0.7696 - val_accuracy: 0.8126\n",
            "Epoch 28/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0547 - accuracy: 0.9809 - val_loss: 0.7919 - val_accuracy: 0.8169\n",
            "Epoch 29/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0562 - accuracy: 0.9802 - val_loss: 0.7886 - val_accuracy: 0.8201\n",
            "Epoch 30/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.0501 - accuracy: 0.9838 - val_loss: 0.8460 - val_accuracy: 0.8062\n",
            "Epoch 31/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0482 - accuracy: 0.9849 - val_loss: 0.8503 - val_accuracy: 0.8105\n",
            "Epoch 32/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0480 - accuracy: 0.9839 - val_loss: 0.8216 - val_accuracy: 0.8169\n",
            "Epoch 33/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0505 - accuracy: 0.9818 - val_loss: 0.8351 - val_accuracy: 0.8094\n",
            "Epoch 34/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0452 - accuracy: 0.9845 - val_loss: 0.9033 - val_accuracy: 0.8180\n",
            "Epoch 35/50\n",
            "526/526 [==============================] - 2s 5ms/step - loss: 0.0472 - accuracy: 0.9855 - val_loss: 0.9324 - val_accuracy: 0.7966\n",
            "Epoch 36/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0462 - accuracy: 0.9843 - val_loss: 0.8999 - val_accuracy: 0.8169\n",
            "Epoch 37/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0428 - accuracy: 0.9865 - val_loss: 0.8995 - val_accuracy: 0.8233\n",
            "Epoch 38/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0409 - accuracy: 0.9859 - val_loss: 0.9414 - val_accuracy: 0.8158\n",
            "Epoch 39/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0432 - accuracy: 0.9857 - val_loss: 0.9452 - val_accuracy: 0.8180\n",
            "Epoch 40/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0432 - accuracy: 0.9848 - val_loss: 0.9698 - val_accuracy: 0.8116\n",
            "Epoch 41/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0386 - accuracy: 0.9874 - val_loss: 0.9630 - val_accuracy: 0.8148\n",
            "Epoch 42/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0389 - accuracy: 0.9876 - val_loss: 0.9553 - val_accuracy: 0.8030\n",
            "Epoch 43/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0397 - accuracy: 0.9867 - val_loss: 0.9420 - val_accuracy: 0.8051\n",
            "Epoch 44/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0391 - accuracy: 0.9867 - val_loss: 0.9900 - val_accuracy: 0.8094\n",
            "Epoch 45/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0350 - accuracy: 0.9882 - val_loss: 1.0156 - val_accuracy: 0.8019\n",
            "Epoch 46/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0384 - accuracy: 0.9871 - val_loss: 0.9545 - val_accuracy: 0.8126\n",
            "Epoch 47/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0327 - accuracy: 0.9895 - val_loss: 1.0490 - val_accuracy: 0.8073\n",
            "Epoch 48/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0378 - accuracy: 0.9875 - val_loss: 1.0098 - val_accuracy: 0.8148\n",
            "Epoch 49/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0372 - accuracy: 0.9864 - val_loss: 1.0063 - val_accuracy: 0.8126\n",
            "Epoch 50/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.0355 - accuracy: 0.9881 - val_loss: 1.0249 - val_accuracy: 0.8105\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.3842 - accuracy: 0.7580\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 50 and Batch Size 32\n",
            "Optimizer Adam Activation relu Units 300 Dropout Rate 0.5\n",
            "\n",
            "Epoch 1/50\n",
            "526/526 [==============================] - 4s 7ms/step - loss: 0.5381 - accuracy: 0.7269 - val_loss: 0.4696 - val_accuracy: 0.7923\n",
            "Epoch 2/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.4723 - accuracy: 0.7790 - val_loss: 0.4513 - val_accuracy: 0.7891\n",
            "Epoch 3/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4388 - accuracy: 0.7980 - val_loss: 0.4411 - val_accuracy: 0.7955\n",
            "Epoch 4/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4149 - accuracy: 0.8146 - val_loss: 0.4275 - val_accuracy: 0.8073\n",
            "Epoch 5/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3853 - accuracy: 0.8307 - val_loss: 0.4219 - val_accuracy: 0.8084\n",
            "Epoch 6/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3646 - accuracy: 0.8394 - val_loss: 0.4181 - val_accuracy: 0.8137\n",
            "Epoch 7/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.3452 - accuracy: 0.8509 - val_loss: 0.4221 - val_accuracy: 0.8266\n",
            "Epoch 8/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3223 - accuracy: 0.8623 - val_loss: 0.4169 - val_accuracy: 0.8094\n",
            "Epoch 9/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2962 - accuracy: 0.8746 - val_loss: 0.4232 - val_accuracy: 0.8169\n",
            "Epoch 10/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2828 - accuracy: 0.8838 - val_loss: 0.4341 - val_accuracy: 0.8158\n",
            "Epoch 11/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2554 - accuracy: 0.8976 - val_loss: 0.4347 - val_accuracy: 0.8255\n",
            "Epoch 12/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2421 - accuracy: 0.9026 - val_loss: 0.4612 - val_accuracy: 0.8084\n",
            "Epoch 13/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2328 - accuracy: 0.9045 - val_loss: 0.4537 - val_accuracy: 0.8191\n",
            "Epoch 14/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2144 - accuracy: 0.9150 - val_loss: 0.4627 - val_accuracy: 0.8201\n",
            "Epoch 15/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2042 - accuracy: 0.9174 - val_loss: 0.4788 - val_accuracy: 0.8244\n",
            "Epoch 16/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1913 - accuracy: 0.9224 - val_loss: 0.4871 - val_accuracy: 0.8201\n",
            "Epoch 17/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.1772 - accuracy: 0.9316 - val_loss: 0.5162 - val_accuracy: 0.8158\n",
            "Epoch 18/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1732 - accuracy: 0.9318 - val_loss: 0.5312 - val_accuracy: 0.8180\n",
            "Epoch 19/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1633 - accuracy: 0.9359 - val_loss: 0.5435 - val_accuracy: 0.8212\n",
            "Epoch 20/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1579 - accuracy: 0.9391 - val_loss: 0.5496 - val_accuracy: 0.8169\n",
            "Epoch 21/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1533 - accuracy: 0.9411 - val_loss: 0.5452 - val_accuracy: 0.8105\n",
            "Epoch 22/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.1475 - accuracy: 0.9435 - val_loss: 0.5662 - val_accuracy: 0.8094\n",
            "Epoch 23/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1407 - accuracy: 0.9458 - val_loss: 0.5822 - val_accuracy: 0.8094\n",
            "Epoch 24/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1341 - accuracy: 0.9494 - val_loss: 0.5951 - val_accuracy: 0.8158\n",
            "Epoch 25/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1282 - accuracy: 0.9517 - val_loss: 0.6066 - val_accuracy: 0.8105\n",
            "Epoch 26/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1235 - accuracy: 0.9532 - val_loss: 0.6324 - val_accuracy: 0.8105\n",
            "Epoch 27/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1227 - accuracy: 0.9528 - val_loss: 0.6399 - val_accuracy: 0.8137\n",
            "Epoch 28/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1208 - accuracy: 0.9545 - val_loss: 0.6506 - val_accuracy: 0.8191\n",
            "Epoch 29/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1148 - accuracy: 0.9556 - val_loss: 0.6517 - val_accuracy: 0.8148\n",
            "Epoch 30/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1147 - accuracy: 0.9566 - val_loss: 0.6475 - val_accuracy: 0.8201\n",
            "Epoch 31/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.1101 - accuracy: 0.9594 - val_loss: 0.6377 - val_accuracy: 0.8062\n",
            "Epoch 32/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.1053 - accuracy: 0.9606 - val_loss: 0.6879 - val_accuracy: 0.8180\n",
            "Epoch 33/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1018 - accuracy: 0.9609 - val_loss: 0.7011 - val_accuracy: 0.8116\n",
            "Epoch 34/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1026 - accuracy: 0.9620 - val_loss: 0.6878 - val_accuracy: 0.8094\n",
            "Epoch 35/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.1000 - accuracy: 0.9642 - val_loss: 0.6901 - val_accuracy: 0.8094\n",
            "Epoch 36/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0993 - accuracy: 0.9632 - val_loss: 0.7043 - val_accuracy: 0.8094\n",
            "Epoch 37/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.0922 - accuracy: 0.9670 - val_loss: 0.7248 - val_accuracy: 0.8084\n",
            "Epoch 38/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0907 - accuracy: 0.9655 - val_loss: 0.7520 - val_accuracy: 0.8116\n",
            "Epoch 39/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0927 - accuracy: 0.9642 - val_loss: 0.7283 - val_accuracy: 0.8180\n",
            "Epoch 40/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0902 - accuracy: 0.9679 - val_loss: 0.7600 - val_accuracy: 0.8009\n",
            "Epoch 41/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0860 - accuracy: 0.9683 - val_loss: 0.7647 - val_accuracy: 0.8137\n",
            "Epoch 42/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0835 - accuracy: 0.9694 - val_loss: 0.7454 - val_accuracy: 0.8084\n",
            "Epoch 43/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0904 - accuracy: 0.9667 - val_loss: 0.7769 - val_accuracy: 0.8126\n",
            "Epoch 44/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.0815 - accuracy: 0.9698 - val_loss: 0.7697 - val_accuracy: 0.8158\n",
            "Epoch 45/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0800 - accuracy: 0.9681 - val_loss: 0.7927 - val_accuracy: 0.8116\n",
            "Epoch 46/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0801 - accuracy: 0.9709 - val_loss: 0.7737 - val_accuracy: 0.8137\n",
            "Epoch 47/50\n",
            "526/526 [==============================] - 4s 7ms/step - loss: 0.0813 - accuracy: 0.9709 - val_loss: 0.7868 - val_accuracy: 0.8223\n",
            "Epoch 48/50\n",
            "526/526 [==============================] - 4s 7ms/step - loss: 0.0771 - accuracy: 0.9705 - val_loss: 0.7899 - val_accuracy: 0.8233\n",
            "Epoch 49/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.0793 - accuracy: 0.9710 - val_loss: 0.8321 - val_accuracy: 0.8169\n",
            "Epoch 50/50\n",
            "526/526 [==============================] - 5s 10ms/step - loss: 0.0739 - accuracy: 0.9726 - val_loss: 0.8638 - val_accuracy: 0.8191\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1.1258 - accuracy: 0.7645\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 50 and Batch Size 32\n",
            "Optimizer Adam Activation relu Units 300 Dropout Rate 0.8\n",
            "\n",
            "Epoch 1/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.5703 - accuracy: 0.7038 - val_loss: 0.4784 - val_accuracy: 0.7827\n",
            "Epoch 2/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.5051 - accuracy: 0.7541 - val_loss: 0.4586 - val_accuracy: 0.7955\n",
            "Epoch 3/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.4823 - accuracy: 0.7703 - val_loss: 0.4515 - val_accuracy: 0.7891\n",
            "Epoch 4/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.4710 - accuracy: 0.7809 - val_loss: 0.4410 - val_accuracy: 0.7987\n",
            "Epoch 5/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4578 - accuracy: 0.7858 - val_loss: 0.4386 - val_accuracy: 0.8019\n",
            "Epoch 6/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4518 - accuracy: 0.7864 - val_loss: 0.4361 - val_accuracy: 0.7998\n",
            "Epoch 7/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4436 - accuracy: 0.7959 - val_loss: 0.4333 - val_accuracy: 0.8073\n",
            "Epoch 8/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4372 - accuracy: 0.7987 - val_loss: 0.4321 - val_accuracy: 0.8030\n",
            "Epoch 9/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.4274 - accuracy: 0.8012 - val_loss: 0.4303 - val_accuracy: 0.8084\n",
            "Epoch 10/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.4192 - accuracy: 0.8100 - val_loss: 0.4255 - val_accuracy: 0.8116\n",
            "Epoch 11/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.4127 - accuracy: 0.8099 - val_loss: 0.4255 - val_accuracy: 0.8148\n",
            "Epoch 12/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.4025 - accuracy: 0.8186 - val_loss: 0.4247 - val_accuracy: 0.8105\n",
            "Epoch 13/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3987 - accuracy: 0.8221 - val_loss: 0.4195 - val_accuracy: 0.8041\n",
            "Epoch 14/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3961 - accuracy: 0.8192 - val_loss: 0.4234 - val_accuracy: 0.8105\n",
            "Epoch 15/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3903 - accuracy: 0.8233 - val_loss: 0.4251 - val_accuracy: 0.8137\n",
            "Epoch 16/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.3818 - accuracy: 0.8287 - val_loss: 0.4250 - val_accuracy: 0.8084\n",
            "Epoch 17/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3795 - accuracy: 0.8322 - val_loss: 0.4237 - val_accuracy: 0.8105\n",
            "Epoch 18/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3723 - accuracy: 0.8332 - val_loss: 0.4195 - val_accuracy: 0.8137\n",
            "Epoch 19/50\n",
            "526/526 [==============================] - 2s 5ms/step - loss: 0.3736 - accuracy: 0.8297 - val_loss: 0.4271 - val_accuracy: 0.8073\n",
            "Epoch 20/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3589 - accuracy: 0.8404 - val_loss: 0.4279 - val_accuracy: 0.8148\n",
            "Epoch 21/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3561 - accuracy: 0.8407 - val_loss: 0.4358 - val_accuracy: 0.8180\n",
            "Epoch 22/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3469 - accuracy: 0.8432 - val_loss: 0.4340 - val_accuracy: 0.8030\n",
            "Epoch 23/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3471 - accuracy: 0.8455 - val_loss: 0.4345 - val_accuracy: 0.8051\n",
            "Epoch 24/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3442 - accuracy: 0.8476 - val_loss: 0.4325 - val_accuracy: 0.8084\n",
            "Epoch 25/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3410 - accuracy: 0.8498 - val_loss: 0.4307 - val_accuracy: 0.8084\n",
            "Epoch 26/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.3368 - accuracy: 0.8512 - val_loss: 0.4266 - val_accuracy: 0.8191\n",
            "Epoch 27/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3334 - accuracy: 0.8517 - val_loss: 0.4304 - val_accuracy: 0.8137\n",
            "Epoch 28/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3253 - accuracy: 0.8592 - val_loss: 0.4391 - val_accuracy: 0.8137\n",
            "Epoch 29/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3288 - accuracy: 0.8551 - val_loss: 0.4365 - val_accuracy: 0.8223\n",
            "Epoch 30/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3222 - accuracy: 0.8590 - val_loss: 0.4419 - val_accuracy: 0.8212\n",
            "Epoch 31/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.3190 - accuracy: 0.8599 - val_loss: 0.4400 - val_accuracy: 0.8137\n",
            "Epoch 32/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3171 - accuracy: 0.8597 - val_loss: 0.4489 - val_accuracy: 0.8158\n",
            "Epoch 33/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.3120 - accuracy: 0.8658 - val_loss: 0.4505 - val_accuracy: 0.8116\n",
            "Epoch 34/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.3124 - accuracy: 0.8651 - val_loss: 0.4471 - val_accuracy: 0.8148\n",
            "Epoch 35/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3007 - accuracy: 0.8686 - val_loss: 0.4507 - val_accuracy: 0.8116\n",
            "Epoch 36/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2976 - accuracy: 0.8703 - val_loss: 0.4524 - val_accuracy: 0.8094\n",
            "Epoch 37/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.3052 - accuracy: 0.8637 - val_loss: 0.4574 - val_accuracy: 0.8137\n",
            "Epoch 38/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2977 - accuracy: 0.8703 - val_loss: 0.4669 - val_accuracy: 0.8116\n",
            "Epoch 39/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2893 - accuracy: 0.8731 - val_loss: 0.4650 - val_accuracy: 0.8116\n",
            "Epoch 40/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2889 - accuracy: 0.8748 - val_loss: 0.4651 - val_accuracy: 0.8169\n",
            "Epoch 41/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.2904 - accuracy: 0.8729 - val_loss: 0.4722 - val_accuracy: 0.8094\n",
            "Epoch 42/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2801 - accuracy: 0.8807 - val_loss: 0.4662 - val_accuracy: 0.8094\n",
            "Epoch 43/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2836 - accuracy: 0.8767 - val_loss: 0.4833 - val_accuracy: 0.8009\n",
            "Epoch 44/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2851 - accuracy: 0.8757 - val_loss: 0.4715 - val_accuracy: 0.8073\n",
            "Epoch 45/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2807 - accuracy: 0.8760 - val_loss: 0.4947 - val_accuracy: 0.8073\n",
            "Epoch 46/50\n",
            "526/526 [==============================] - 3s 5ms/step - loss: 0.2820 - accuracy: 0.8804 - val_loss: 0.4812 - val_accuracy: 0.8062\n",
            "Epoch 47/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2778 - accuracy: 0.8789 - val_loss: 0.4856 - val_accuracy: 0.8137\n",
            "Epoch 48/50\n",
            "526/526 [==============================] - 3s 6ms/step - loss: 0.2668 - accuracy: 0.8811 - val_loss: 0.4939 - val_accuracy: 0.8169\n",
            "Epoch 49/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2719 - accuracy: 0.8782 - val_loss: 0.4795 - val_accuracy: 0.8105\n",
            "Epoch 50/50\n",
            "526/526 [==============================] - 2s 4ms/step - loss: 0.2661 - accuracy: 0.8830 - val_loss: 0.4987 - val_accuracy: 0.8158\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6134 - accuracy: 0.7709\n",
            "\n",
            "------------------------------------------------------------\n",
            "0.7708779573440552\n"
          ]
        }
      ],
      "source": [
        "epochs = [50]\n",
        "batch_sizes = [32]\n",
        "\n",
        "shape=(300)\n",
        "max_acc = 0\n",
        "\n",
        "X_test_1, X_test_2, y_test = test\n",
        "\n",
        "for epoch in epochs:\n",
        "  for batch_size in batch_sizes:\n",
        "    for hyperparameters in hyperparameters_list:\n",
        "      optimizer, activation, units, dropout_rate = hyperparameters\n",
        "      print(\"Epoch {} and Batch Size {}\".format(epoch, batch_size))\n",
        "      print(\"Optimizer {} Activation {} Units {} Dropout Rate {}\".format(optimizer, activation, units, dropout_rate))\n",
        "      print()\n",
        "      classifier.train(train, val, epoch, batch_size, hyperparameters, shape)\n",
        "      model = classifier.get_model()\n",
        "      loss, accuracy = model.evaluate([X_test_1, X_test_2], y_test)\n",
        "      if accuracy >= max_acc:\n",
        "        max_acc = accuracy\n",
        "        best_model = model\n",
        "      print()\n",
        "      print(\"------------------------------------------------------------\")\n",
        "    print(max_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAPITrDbA6Bh"
      },
      "outputs": [],
      "source": [
        "def calculate_average_peformance(test):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4y5mJ3KxGur"
      },
      "outputs": [],
      "source": [
        "del res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS0V6svMx5Lo",
        "outputId": "b11a8393-41b9-43a0-a6d6-ca38d8f4f4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LASER'...\n",
            "remote: Enumerating objects: 938, done.\u001b[K\n",
            "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 938 (delta 70), reused 125 (delta 55), pack-reused 787\u001b[K\n",
            "Receiving objects: 100% (938/938), 2.90 MiB | 11.64 MiB/s, done.\n",
            "Resolving deltas: 100% (373/373), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/LASER.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7Gd4g5xx5pa",
        "outputId": "7c951f9a-9270-4d1a-a45b-c16b43d9f752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - creating directory /content/LASER/tools-external\n",
            "Installing external tools\n",
            " - creating directory /content/LASER/tools-external/moses-tokenizer/tokenizer\n",
            " - download tokenizer/tokenizer.perl\n",
            " - download tokenizer/detokenizer.perl\n",
            " - download tokenizer/normalize-punctuation.perl\n",
            " - download tokenizer/remove-non-printing-char.perl\n",
            " - download tokenizer/deescape-special-chars.perl\n",
            " - download tokenizer/lowercase.perl\n",
            " - download tokenizer/basic-protected-patterns\n",
            " - creating directory /content/LASER/tools-external/moses-tokenizer/share/nonbreaking_prefixes\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ca\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.cs\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.de\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.el\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.en\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.es\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.fi\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.fr\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ga\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.hu\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.is\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.it\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.lt\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.lv\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.nl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.pl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.pt\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ro\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ru\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sk\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sv\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ta\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.yue\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.zh\n",
            " - download fastBPE software from github\n",
            "--2023-06-16 01:24:05--  https://github.com/glample/fastBPE/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/glample/fastBPE/zip/refs/heads/master [following]\n",
            "--2023-06-16 01:24:05--  https://codeload.github.com/glample/fastBPE/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: master.zip\n",
            "\n",
            "master.zip              [ <=>                ]   9.46K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-16 01:24:05 (10.8 MB/s) - master.zip saved [9684]\n",
            "\n",
            "Archive:  master.zip\n",
            "036711f8fdc3265d64e8e123a0761be12c5a8e74\n",
            "   creating: fastBPE-master/\n",
            "  inflating: fastBPE-master/LICENSE  \n",
            "  inflating: fastBPE-master/MANIFEST.in  \n",
            "  inflating: fastBPE-master/README.md  \n",
            "   creating: fastBPE-master/fastBPE/\n",
            "  inflating: fastBPE-master/fastBPE/fastBPE.hpp  \n",
            "  inflating: fastBPE-master/fastBPE/fastBPE.pyx  \n",
            "  inflating: fastBPE-master/fastBPE/main.cc  \n",
            "  inflating: fastBPE-master/setup.py  \n",
            " - compiling\n",
            "Compiling fastBPE/fastBPE.pyx because it changed.\n",
            "[1/1] Cythonizing fastBPE/fastBPE.pyx\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating fastBPE.egg-info\n",
            "writing fastBPE.egg-info/PKG-INFO\n",
            "writing dependency_links to fastBPE.egg-info/dependency_links.txt\n",
            "writing top-level names to fastBPE.egg-info/top_level.txt\n",
            "writing manifest file 'fastBPE.egg-info/SOURCES.txt'\n",
            "reading manifest file 'fastBPE.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'fastBPE.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "running build_ext\n",
            "building 'fastBPE' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/fastBPE\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -IfastBPE -I/usr/include/python3.10 -c fastBPE/fastBPE.cpp -o build/temp.linux-x86_64-cpython-310/fastBPE/fastBPE.o -std=c++11 -Ofast -pthread\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fastBPE/fastBPE.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-310/fastBPE.cpython-310-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/fastBPE.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for fastBPE.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/fastBPE.py to fastBPE.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fastBPE.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fastBPE.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fastBPE.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fastBPE.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.fastBPE.cpython-310: module references __file__\n",
            "creating dist\n",
            "creating 'dist/fastBPE-0.1.1-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing fastBPE-0.1.1-py3.10-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/fastBPE-0.1.1-py3.10-linux-x86_64.egg\n",
            "Extracting fastBPE-0.1.1-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding fastBPE 0.1.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/fastBPE-0.1.1-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for fastBPE==0.1.1\n",
            "Finished processing dependencies for fastBPE==0.1.1\n",
            " - download sentencepiece from github\n",
            "--2023-06-16 01:24:19--  https://github.com/google/sentencepiece/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/google/sentencepiece/zip/refs/heads/master [following]\n",
            "--2023-06-16 01:24:19--  https://codeload.github.com/google/sentencepiece/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: master.zip\n",
            "\n",
            "master.zip              [       <=>          ]  11.55M  4.18MB/s    in 2.8s    \n",
            "\n",
            "2023-06-16 01:24:22 (4.18 MB/s) - master.zip saved [12107123]\n",
            "\n",
            "Archive:  master.zip\n",
            "7b694e4bdb8d08be405f3370a6d147d9b8fcea76\n",
            "   creating: sentencepiece-master/\n",
            "   creating: sentencepiece-master/.github/\n",
            "   creating: sentencepiece-master/.github/workflows/\n",
            "  inflating: sentencepiece-master/.github/workflows/cifuzz.yml  \n",
            "  inflating: sentencepiece-master/.github/workflows/cmake.yml  \n",
            "  inflating: sentencepiece-master/.github/workflows/cross_build.yml  \n",
            "  inflating: sentencepiece-master/.github/workflows/wheel.yml  \n",
            "  inflating: sentencepiece-master/.gitignore  \n",
            "  inflating: sentencepiece-master/CMakeLists.txt  \n",
            "  inflating: sentencepiece-master/CONTRIBUTING.md  \n",
            "  inflating: sentencepiece-master/LICENSE  \n",
            "  inflating: sentencepiece-master/README.md  \n",
            " extracting: sentencepiece-master/VERSION.txt  \n",
            "   creating: sentencepiece-master/cmake/\n",
            "  inflating: sentencepiece-master/cmake/ios.toolchain.cmake  \n",
            "  inflating: sentencepiece-master/config.h.in  \n",
            "   creating: sentencepiece-master/data/\n",
            "  inflating: sentencepiece-master/data/Scripts.txt  \n",
            "  inflating: sentencepiece-master/data/botchan.txt  \n",
            "  inflating: sentencepiece-master/data/extract_headers.pl  \n",
            "  inflating: sentencepiece-master/data/gen_spec_parser.pl  \n",
            "  inflating: sentencepiece-master/data/gen_unicode_scripts_code.pl  \n",
            "  inflating: sentencepiece-master/data/ids_denorm.tsv  \n",
            "  inflating: sentencepiece-master/data/ids_norm.tsv  \n",
            "  inflating: sentencepiece-master/data/nfkc.tsv  \n",
            "  inflating: sentencepiece-master/data/nfkc_cf.tsv  \n",
            "  inflating: sentencepiece-master/data/nfkd.tsv  \n",
            "  inflating: sentencepiece-master/data/nmt_nfkc.tsv  \n",
            "  inflating: sentencepiece-master/data/nmt_nfkc_cf.tsv  \n",
            "  inflating: sentencepiece-master/data/wagahaiwa_nekodearu.txt  \n",
            "   creating: sentencepiece-master/doc/\n",
            "  inflating: sentencepiece-master/doc/api.md  \n",
            "  inflating: sentencepiece-master/doc/experiments.md  \n",
            "  inflating: sentencepiece-master/doc/normalization.md  \n",
            "  inflating: sentencepiece-master/doc/options.md  \n",
            "  inflating: sentencepiece-master/doc/special_symbols.md  \n",
            "   creating: sentencepiece-master/python/\n",
            " extracting: sentencepiece-master/python/.gitignore  \n",
            "  inflating: sentencepiece-master/python/MANIFEST.in  \n",
            "  inflating: sentencepiece-master/python/README.md  \n",
            "  inflating: sentencepiece-master/python/add_new_vocab.ipynb  \n",
            "  inflating: sentencepiece-master/python/build_bundled.sh  \n",
            "  inflating: sentencepiece-master/python/build_sdist.sh  \n",
            "  inflating: sentencepiece-master/python/sentencepiece_python_module_example.ipynb  \n",
            " extracting: sentencepiece-master/python/setup.cfg  \n",
            "  inflating: sentencepiece-master/python/setup.py  \n",
            "   creating: sentencepiece-master/python/src/\n",
            "   creating: sentencepiece-master/python/src/sentencepiece/\n",
            "  inflating: sentencepiece-master/python/src/sentencepiece/__init__.py  \n",
            " extracting: sentencepiece-master/python/src/sentencepiece/_version.py  \n",
            "  inflating: sentencepiece-master/python/src/sentencepiece/sentencepiece.i  \n",
            "  inflating: sentencepiece-master/python/src/sentencepiece/sentencepiece_model_pb2.py  \n",
            "  inflating: sentencepiece-master/python/src/sentencepiece/sentencepiece_pb2.py  \n",
            "  inflating: sentencepiece-master/python/src/sentencepiece/sentencepiece_wrap.cxx  \n",
            "   creating: sentencepiece-master/python/test/\n",
            " extracting: sentencepiece-master/python/test/__init__.py  \n",
            "    linking: sentencepiece-master/python/test/botchan.txt  -> ../../data/botchan.txt \n",
            "  inflating: sentencepiece-master/python/test/sentencepiece_test.py  \n",
            "  inflating: sentencepiece-master/python/test/test_ja_model.model  \n",
            "  inflating: sentencepiece-master/python/test/test_model.model  \n",
            "  inflating: sentencepiece-master/sentencepiece.pc.in  \n",
            "   creating: sentencepiece-master/src/\n",
            "  inflating: sentencepiece-master/src/CMakeLists.txt  \n",
            "  inflating: sentencepiece-master/src/bpe_model.cc  \n",
            "  inflating: sentencepiece-master/src/bpe_model.h  \n",
            "  inflating: sentencepiece-master/src/bpe_model_test.cc  \n",
            "  inflating: sentencepiece-master/src/bpe_model_trainer.cc  \n",
            "  inflating: sentencepiece-master/src/bpe_model_trainer.h  \n",
            "  inflating: sentencepiece-master/src/bpe_model_trainer_test.cc  \n",
            "  inflating: sentencepiece-master/src/builder.cc  \n",
            "  inflating: sentencepiece-master/src/builder.h  \n",
            "  inflating: sentencepiece-master/src/builder_test.cc  \n",
            "   creating: sentencepiece-master/src/builtin_pb/\n",
            "  inflating: sentencepiece-master/src/builtin_pb/sentencepiece.pb.cc  \n",
            "  inflating: sentencepiece-master/src/builtin_pb/sentencepiece.pb.h  \n",
            "  inflating: sentencepiece-master/src/builtin_pb/sentencepiece_model.pb.cc  \n",
            "  inflating: sentencepiece-master/src/builtin_pb/sentencepiece_model.pb.h  \n",
            "  inflating: sentencepiece-master/src/char_model.cc  \n",
            "  inflating: sentencepiece-master/src/char_model.h  \n",
            "  inflating: sentencepiece-master/src/char_model_test.cc  \n",
            "  inflating: sentencepiece-master/src/char_model_trainer.cc  \n",
            "  inflating: sentencepiece-master/src/char_model_trainer.h  \n",
            "  inflating: sentencepiece-master/src/char_model_trainer_test.cc  \n",
            "  inflating: sentencepiece-master/src/common.h  \n",
            "  inflating: sentencepiece-master/src/compile_charsmap_main.cc  \n",
            "  inflating: sentencepiece-master/src/error.cc  \n",
            "  inflating: sentencepiece-master/src/filesystem.cc  \n",
            "  inflating: sentencepiece-master/src/filesystem.h  \n",
            "  inflating: sentencepiece-master/src/filesystem_test.cc  \n",
            "  inflating: sentencepiece-master/src/freelist.h  \n",
            "  inflating: sentencepiece-master/src/freelist_test.cc  \n",
            "  inflating: sentencepiece-master/src/init.h  \n",
            "  inflating: sentencepiece-master/src/init_test.cc  \n",
            "  inflating: sentencepiece-master/src/model_factory.cc  \n",
            "  inflating: sentencepiece-master/src/model_factory.h  \n",
            "  inflating: sentencepiece-master/src/model_factory_test.cc  \n",
            "  inflating: sentencepiece-master/src/model_interface.cc  \n",
            "  inflating: sentencepiece-master/src/model_interface.h  \n",
            "  inflating: sentencepiece-master/src/model_interface_test.cc  \n",
            "  inflating: sentencepiece-master/src/normalization_rule.h  \n",
            "  inflating: sentencepiece-master/src/normalizer.cc  \n",
            "  inflating: sentencepiece-master/src/normalizer.h  \n",
            "  inflating: sentencepiece-master/src/normalizer_test.cc  \n",
            "  inflating: sentencepiece-master/src/pretokenizer_for_training.cc  \n",
            "  inflating: sentencepiece-master/src/pretokenizer_for_training.h  \n",
            "  inflating: sentencepiece-master/src/pretokenizer_for_training_test.cc  \n",
            "  inflating: sentencepiece-master/src/sentencepiece.proto  \n",
            "  inflating: sentencepiece-master/src/sentencepiece_model.proto  \n",
            "  inflating: sentencepiece-master/src/sentencepiece_processor.cc  \n",
            "  inflating: sentencepiece-master/src/sentencepiece_processor.h  \n",
            "  inflating: sentencepiece-master/src/sentencepiece_processor_test.cc  \n",
            "  inflating: sentencepiece-master/src/sentencepiece_trainer.cc  \n",
            "  inflating: sentencepiece-master/src/sentencepiece_trainer.h  \n",
            "  inflating: sentencepiece-master/src/sentencepiece_trainer_test.cc  \n",
            "  inflating: sentencepiece-master/src/spec_parser.h  \n",
            "  inflating: sentencepiece-master/src/spm_decode_main.cc  \n",
            "  inflating: sentencepiece-master/src/spm_encode_main.cc  \n",
            "  inflating: sentencepiece-master/src/spm_export_vocab_main.cc  \n",
            "  inflating: sentencepiece-master/src/spm_normalize_main.cc  \n",
            "  inflating: sentencepiece-master/src/spm_train_main.cc  \n",
            "  inflating: sentencepiece-master/src/test_main.cc  \n",
            "  inflating: sentencepiece-master/src/testharness.cc  \n",
            "  inflating: sentencepiece-master/src/testharness.h  \n",
            "  inflating: sentencepiece-master/src/trainer_factory.cc  \n",
            "  inflating: sentencepiece-master/src/trainer_factory.h  \n",
            "  inflating: sentencepiece-master/src/trainer_factory_test.cc  \n",
            "  inflating: sentencepiece-master/src/trainer_interface.cc  \n",
            "  inflating: sentencepiece-master/src/trainer_interface.h  \n",
            "  inflating: sentencepiece-master/src/trainer_interface_test.cc  \n",
            "  inflating: sentencepiece-master/src/unicode_script.cc  \n",
            "  inflating: sentencepiece-master/src/unicode_script.h  \n",
            "  inflating: sentencepiece-master/src/unicode_script_map.h  \n",
            "  inflating: sentencepiece-master/src/unicode_script_test.cc  \n",
            "  inflating: sentencepiece-master/src/unigram_model.cc  \n",
            "  inflating: sentencepiece-master/src/unigram_model.h  \n",
            "  inflating: sentencepiece-master/src/unigram_model_test.cc  \n",
            "  inflating: sentencepiece-master/src/unigram_model_trainer.cc  \n",
            "  inflating: sentencepiece-master/src/unigram_model_trainer.h  \n",
            "  inflating: sentencepiece-master/src/unigram_model_trainer_test.cc  \n",
            "  inflating: sentencepiece-master/src/util.cc  \n",
            "  inflating: sentencepiece-master/src/util.h  \n",
            "  inflating: sentencepiece-master/src/util_test.cc  \n",
            "  inflating: sentencepiece-master/src/word_model.cc  \n",
            "  inflating: sentencepiece-master/src/word_model.h  \n",
            "  inflating: sentencepiece-master/src/word_model_test.cc  \n",
            "  inflating: sentencepiece-master/src/word_model_trainer.cc  \n",
            "  inflating: sentencepiece-master/src/word_model_trainer.h  \n",
            "  inflating: sentencepiece-master/src/word_model_trainer_test.cc  \n",
            "   creating: sentencepiece-master/third_party/\n",
            "  inflating: sentencepiece-master/third_party/CMakeLists.txt  \n",
            "   creating: sentencepiece-master/third_party/absl/\n",
            "  inflating: sentencepiece-master/third_party/absl/LICENSE  \n",
            "   creating: sentencepiece-master/third_party/absl/container/\n",
            "  inflating: sentencepiece-master/third_party/absl/container/flat_hash_map.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/container/flat_hash_set.h  \n",
            "   creating: sentencepiece-master/third_party/absl/flags/\n",
            "  inflating: sentencepiece-master/third_party/absl/flags/flag.cc  \n",
            "  inflating: sentencepiece-master/third_party/absl/flags/flag.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/flags/parse.h  \n",
            "   creating: sentencepiece-master/third_party/absl/memory/\n",
            "  inflating: sentencepiece-master/third_party/absl/memory/memory.h  \n",
            "   creating: sentencepiece-master/third_party/absl/random/\n",
            "  inflating: sentencepiece-master/third_party/absl/random/distributions.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/random/random.h  \n",
            "   creating: sentencepiece-master/third_party/absl/strings/\n",
            "  inflating: sentencepiece-master/third_party/absl/strings/ascii.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/match.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/numbers.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/str_cat.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/str_format.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/str_join.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/str_replace.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/str_split.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/string_view.h  \n",
            "  inflating: sentencepiece-master/third_party/absl/strings/strip.h  \n",
            "   creating: sentencepiece-master/third_party/darts_clone/\n",
            "  inflating: sentencepiece-master/third_party/darts_clone/LICENSE  \n",
            "  inflating: sentencepiece-master/third_party/darts_clone/darts.h  \n",
            "   creating: sentencepiece-master/third_party/esaxx/\n",
            "  inflating: sentencepiece-master/third_party/esaxx/LICENSE  \n",
            "  inflating: sentencepiece-master/third_party/esaxx/esa.hxx  \n",
            "  inflating: sentencepiece-master/third_party/esaxx/sais.hxx  \n",
            "   creating: sentencepiece-master/third_party/protobuf-lite/\n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/LICENSE  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/arena.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/arenastring.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/bytestream.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/coded_stream.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/common.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/extension_set.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/generated_enum_util.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/generated_message_table_driven_lite.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/generated_message_util.cc  \n",
            "   creating: sentencepiece-master/third_party/protobuf-lite/google/\n",
            "   creating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/\n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/any.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/arena.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/arena_impl.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/arenastring.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/descriptor.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/extension_set.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/extension_set_inl.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/generated_enum_reflection.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/generated_enum_util.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/generated_message_table_driven.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/generated_message_table_driven_lite.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/generated_message_util.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/has_bits.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/implicit_weak_message.h  \n",
            "   creating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/io/\n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/io/coded_stream.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/io/io_win32.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/io/zero_copy_stream.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/io/zero_copy_stream_impl.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/io/zero_copy_stream_impl_lite.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/map.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/map_entry_lite.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/map_field_lite.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/map_type_handler.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/message_lite.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/metadata_lite.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/parse_context.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/port.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/port_def.inc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/port_undef.inc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/repeated_field.h  \n",
            "   creating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/\n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/bytestream.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/callback.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/casts.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/common.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/hash.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/int128.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/logging.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/macros.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/map_util.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/mutex.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/once.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/platform_macros.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/port.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/status.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/statusor.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/stl_util.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/stringpiece.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/stringprintf.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/strutil.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/stubs/time.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/unknown_field_set.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/google/protobuf/wire_format_lite.h  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/implicit_weak_message.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/int128.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/io_win32.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/message_lite.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/parse_context.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/repeated_field.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/status.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/statusor.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/stringpiece.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/stringprintf.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/structurally_valid.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/strutil.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/time.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/wire_format_lite.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/zero_copy_stream.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/zero_copy_stream_impl.cc  \n",
            "  inflating: sentencepiece-master/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc  \n",
            "finishing deferred symbolic links:\n",
            "  sentencepiece-master/python/test/botchan.txt -> ../../data/botchan.txt\n",
            " - building code \n",
            "-- VERSION: 0.2.00\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Not Found TCMalloc: TCMALLOC_LIB-NOTFOUND\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/LASER/tools-external/sentencepiece-master/build\n",
            "[  0%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/builder.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/trainer_factory.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/unicode_script.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/trainer_interface.cc.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:495\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function \u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K,\n",
            "    inlined from \u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K at \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from \u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K at \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:85:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:34:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K-Wstringop-overflow=\u001b[m\u001b[K]\n",
            "   34 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len, __bos0 (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/unigram_model_trainer.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/word_model_trainer.cc.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/char_model_trainer.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/bpe_model_trainer.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/sentencepiece_trainer.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/pretokenizer_for_training.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:495\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function \u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K,\n",
            "    inlined from \u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K at \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from \u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K at \u001b[01m\u001b[K/content/LASER/tools-external/sentencepiece-master/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:85:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:34:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K-Wstringop-overflow=\u001b[m\u001b[K]\n",
            "   34 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len, __bos0 (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/char_model.cc.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX static library libsentencepiece_train.a\u001b[0m\n",
            "[ 57%] Built target sentencepiece_train-static\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/error.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/char_model.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/filesystem.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/model_factory.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/error.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/filesystem.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/model_interface.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/model_factory.cc.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/model_interface.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/normalizer.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/normalizer.cc.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/util.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/word_model.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/util.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/word_model.cc.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX static library libsentencepiece.a\u001b[0m\n",
            "[ 81%] Built target sentencepiece-static\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece.so\u001b[0m\n",
            "[ 81%] Built target sentencepiece\n",
            "[ 82%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_decode.dir/spm_decode_main.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_factory.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/unicode_script.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/builder.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_interface.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_encode.dir/spm_encode_main.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_export_vocab.dir/spm_export_vocab_main.cc.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/unigram_model_trainer.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/char_model_trainer.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/word_model_trainer.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable spm_export_vocab\u001b[0m\n",
            "[ 92%] Built target spm_export_vocab\n",
            "[ 92%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/bpe_model_trainer.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/sentencepiece_trainer.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable spm_decode\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/pretokenizer_for_training.cc.o\u001b[0m\n",
            "[ 94%] Built target spm_decode\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable spm_encode\u001b[0m\n",
            "[ 95%] Built target spm_encode\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece_train.so\u001b[0m\n",
            "[ 96%] Built target sentencepiece_train\n",
            "[ 98%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_normalize.dir/spm_normalize_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_train.dir/spm_train_main.cc.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable spm_normalize\u001b[0m\n",
            "[ 99%] Built target spm_normalize\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable spm_train\u001b[0m\n",
            "[100%] Built target spm_train\n",
            "\n",
            "automatic installation of the Japanese tokenizer mecab may be tricky\n",
            "Please install it manually from https://github.com/taku910/mecab\n",
            "\n",
            "The installation directory should be /content/LASER/tools-external/mecab\n",
            "\n",
            "Downloading networks\n",
            " - creating directory /content/LASER/models\n",
            " - bilstm.eparl21.2018-11-19.pt\n",
            " - eparl21.fcodes\n",
            " - eparl21.fvocab\n",
            " - bilstm.93langs.2018-12-26.pt\n",
            " - 93langs.fcodes\n",
            " - 93langs.fvocab\n",
            "LASER/: LASER/: Is a directory\n"
          ]
        }
      ],
      "source": [
        "!bash LASER/install_external_tools.sh\n",
        "!bash LASER/install_models.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HG7JJO_zVMw",
        "outputId": "d490b0ad-d826-473c-b9cc-3aa2036af963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory for model download: /content\n",
            "Downloading networks...\n",
            " - https://dl.fbaipublicfiles.com/nllb/laser/laser2.pt\n",
            " - https://dl.fbaipublicfiles.com/nllb/laser/laser2.spm\n",
            " - https://dl.fbaipublicfiles.com/nllb/laser/laser2.cvocab\n",
            " - https://dl.fbaipublicfiles.com/nllb/laser/laser3-ace_Latn.v1.pt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!bash LASER/nllb/download_models.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtV25mdh0gCz"
      },
      "outputs": [],
      "source": [
        "!mv laser2.pt LASER/nllb/laser2.pt\n",
        "!mv laser2.spm LASER/nllb/laser2.spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMr0NvwxoKQ9"
      },
      "outputs": [],
      "source": [
        "amin = {'haboo': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G-Dt9fT04s7"
      },
      "outputs": [],
      "source": [
        "empty_keys = [key for key, value in amin.items() if len(value) == 0]\n",
        "for key in empty_keys:\n",
        "  del amin[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmYqYboCoP_F"
      },
      "outputs": [],
      "source": [
        "for key, value in amin.items():\n",
        "  print(\"{} is {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPsHcU89oYrJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}